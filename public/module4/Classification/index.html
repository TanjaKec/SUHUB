<!DOCTYPE html>
<html lang="en" class="js csstransforms3d">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Hugo 0.78.2" />
    <meta name="description" content="Exploratory Analysis in R">
<meta name="author" content="Tatjana Kecojevic">

    <link rel="shortcut icon" href="/images/favicon.png" type="image/x-icon" />

    <title>Classification :: Exploratory Analysis in R</title>

    
    <link href="/css/nucleus.css?1638429882" rel="stylesheet">
    <link href="/css/fontawesome-all.min.css?1638429882" rel="stylesheet">
    <link href="/css/hybrid.css?1638429882" rel="stylesheet">
    <link href="/css/featherlight.min.css?1638429882" rel="stylesheet">
    <link href="/css/perfect-scrollbar.min.css?1638429882" rel="stylesheet">
    <link href="/css/auto-complete.css?1638429882" rel="stylesheet">
    <link href="/css/atom-one-dark-reasonable.css?1638429882" rel="stylesheet">
    <link href="/css/theme.css?1638429882" rel="stylesheet">
    <link href="/css/hugo-theme.css?1638429882" rel="stylesheet">
    

    <script src="/js/jquery-3.3.1.min.js?1638429882"></script>

    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"> </script>

    <style>
      :root #header + #content > #left > #rlblock_left{
          display:none !important;
      }
      
    </style>
    
  </head>
  <body class="" data-url="/module4/classification/">
    <nav id="sidebar" class="showVisitedLinks">



  <div id="header-wrapper">
    <div id="header">
      <!DOCTYPE html>
<html>
<body>
<IMG SRC="/images/logo.png" ALT="DataTeka" WIDTH=100 HEIGHT=100>

</body>
</html>


    </div>
    
        <div class="searchbox">
    <label for="search-by"><i class="fas fa-search"></i></label>
    <input data-search-input id="search-by" type="search" placeholder="Search...">
    <span data-search-clear=""><i class="fas fa-times"></i></span>
</div>

<script type="text/javascript" src="/js/lunr.min.js?1638429882"></script>
<script type="text/javascript" src="/js/auto-complete.js?1638429882"></script>
<script type="text/javascript">
    
        var baseurl = "\/";
    
</script>
<script type="text/javascript" src="/js/search.js?1638429882"></script>

    
  </div>

    <div class="highlightable">
    <ul class="topics">

        
          
          


 
  
    
    <li data-nav-id="/general/" title="About the course" class="dd-item 
        
        
        
        ">
      <a href="/general/">
          <b>1. </b>About the course
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            


 
  
    
    <li data-nav-id="/general/whatlearn/" title="General Overview" class="dd-item 
        
        
        
        ">
      <a href="/general/whatlearn/">
          General Overview
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/general/instructor/" title="Meet the Instructor" class="dd-item 
        
        
        
        ">
      <a href="/general/instructor/">
          Meet the Instructor
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/general/syllabus/" title="Indicative Syllabus" class="dd-item 
        
        
        
        ">
      <a href="/general/syllabus/">
          Indicative Syllabus
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          


 
  
    
    <li data-nav-id="/module1/" title="Module 1" class="dd-item 
        
        
        
        ">
      <a href="/module1/">
          <b>2. </b>Module 1
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            


 
  
    
    <li data-nav-id="/module1/whatisr/" title="What is R?" class="dd-item 
        
        
        
        ">
      <a href="/module1/whatisr/">
          What is R?
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module1/installr/" title="Install R/RStudio" class="dd-item 
        
        
        
        ">
      <a href="/module1/installr/">
          Install R
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module1/git-and-github/" title="Git and GitHub" class="dd-item 
        
        
        
        ">
      <a href="/module1/git-and-github/">
          Git and GitHub
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module1/rstudioide/" title="RStudio IDE" class="dd-item 
        
        
        
        ">
      <a href="/module1/rstudioide/">
          RStudio IDE
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module1/user/" title="How to use R?" class="dd-item 
        
        
        
        ">
      <a href="/module1/user/">
          How to use R?
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module1/statsconcepts/" title="Basic Stats Concepts" class="dd-item 
        
        
        
        ">
      <a href="/module1/statsconcepts/">
          Basic Stats Concepts
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module1/datatypes/" title="Data Types" class="dd-item 
        
        
        
        ">
      <a href="/module1/datatypes/">
          Data Types
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module1/importexport/" title="Import Data to R" class="dd-item 
        
        
        
        ">
      <a href="/module1/importexport/">
          Import Data to R
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module1/rmarkdown/" title="Reproducibility" class="dd-item 
        
        
        
        ">
      <a href="/module1/rmarkdown/">
          Reproducibility
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          


 
  
    
    <li data-nav-id="/module2/" title="Module 2" class="dd-item 
        
        
        
        ">
      <a href="/module2/">
          <b>3. </b>Module 2
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            


 
  
    
    <li data-nav-id="/module2/datawrangling/" title="Data Wrangling" class="dd-item 
        
        
        
        ">
      <a href="/module2/datawrangling/">
          Data Wrangling
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module2/principlesvisualisation/" title="Principles of Visualisation" class="dd-item 
        
        
        
        ">
      <a href="/module2/principlesvisualisation/">
          Principles of Visualisation
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module2/visualisation/" title="Data Visualisation" class="dd-item 
        
        
        
        ">
      <a href="/module2/visualisation/">
          Data Visualisation
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module2/spatialda/" title="Spatial Data Analysis" class="dd-item 
        
        
        
        ">
      <a href="/module2/spatialda/">
          Spatial Data Analysis
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          


 
  
    
    <li data-nav-id="/module3/" title="Module 3" class="dd-item 
        
        
        
        ">
      <a href="/module3/">
          <b>4. </b>Module 3
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            


 
  
    
    <li data-nav-id="/module3/damethodology/" title="Data Analysis Methodology" class="dd-item 
        
        
        
        ">
      <a href="/module3/damethodology/">
          Data Analysis Methodology
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module3/mva/" title="t-test &amp; one-way ANOVA" class="dd-item 
        
        
        
        ">
      <a href="/module3/mva/">
          t-test &amp; one-way ANOVA
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module3/mvm/" title="Regression" class="dd-item 
        
        
        
        ">
      <a href="/module3/mvm/">
          Regression
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
          


 
  
    
    <li data-nav-id="/module4/" title="Module 4" class="dd-item 
        parent
        
        
        ">
      <a href="/module4/">
          <b>5. </b>Module 4
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
      
        <ul>
          
          
            
          
          
          
        
          
            
            


 
  
    
    <li data-nav-id="/module4/what_is_ml/" title="Machine Learning" class="dd-item 
        
        
        
        ">
      <a href="/module4/what_is_ml/">
          Machine Learning
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module4/resampling/" title="Resampling" class="dd-item 
        
        
        
        ">
      <a href="/module4/resampling/">
          Resampling
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module4/mlregression/" title="Multiple Regression" class="dd-item 
        
        
        
        ">
      <a href="/module4/mlregression/">
          Multiple Regression
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module4/subsets/" title="Subset Variable Selection" class="dd-item 
        
        
        
        ">
      <a href="/module4/subsets/">
          Subset Variable Selection
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
            
            


 
  
    
    <li data-nav-id="/module4/classification/" title="Classification" class="dd-item 
        parent
        active
        
        ">
      <a href="/module4/classification/">
          Classification
          
            <i class="fas fa-check read-icon"></i>
          
      </a>
      
              
    </li>
  
 

            
          
        
        </ul>
              
    </li>
  
 

          
         
    </ul>

    
    
      <section id="shortcuts">
        <h3>More</h3>
        <ul>
          
              <li> 
                  <a class="padding" href="https://github.com/TanjaKec"><i class='fab fa-fw fa-github'></i>GitHub</a>
              </li>
          
        </ul>
      </section>
    

    
    <section id="prefooter">
      <hr/>
      <ul>
      
      
      
        <li><a class="padding" href="#" data-clear-history-toggle=""><i class="fas fa-history fa-fw"></i> Clear History</a></li>
      
      </ul>
    </section>
    
    <section id="footer">
      <center>
    
    <a class="github-button" href="https://github.com/matcornic/hugo-theme-learn/archive/master.zip" data-icon="octicon-cloud-download" aria-label="Download matcornic/hugo-theme-learn on GitHub">Download</a>

    
    <a class="github-button" href="https://github.com/matcornic/hugo-theme-learn" data-icon="octicon-star" data-show-count="true" aria-label="Star matcornic/hugo-theme-learn on GitHub">Star</a>

    
    <a class="github-button" href="https://github.com/matcornic/hugo-theme-learn/fork" data-icon="octicon-repo-forked" data-show-count="true" aria-label="Fork matcornic/hugo-theme-learn on GitHub">Fork</a>

    <p>Built with <a href="https://github.com/matcornic/hugo-theme-learn"><i class="fas fa-heart"></i></a> from <a href="https://getgrav.org">Grav</a> and <a href="https://gohugo.io/">Hugo</a></p>
</center>

<script async defer src="https://buttons.github.io/buttons.js"></script>

    </section>
  </div>
</nav>





        <section id="body">
        <div id="overlay"></div>
        <div class="padding highlightable">
              
              <div>
                <div id="top-bar">
                
                  
                  
                  
                  <div id="top-github-link">
                    <a class="github-link" title='Edit this page' href="https://github.com/matcornic/hugo-theme-learn/edit/master/exampleSite/content/module4/Classification/_index.en.html" target="blank">
                      <i class="fas fa-code-branch"></i>
                      <span id="top-github-link-text">Edit this page</span>
                    </a>
                  </div>
                  
                
                
                <div id="breadcrumbs" itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span id="sidebar-toggle-span">
                        <a href="#" id="sidebar-toggle" data-sidebar-toggle="">
                          <i class="fas fa-bars"></i>
                        </a>
                    </span>
                  
                  <span id="toc-menu"><i class="fas fa-list-alt"></i></span>
                  
                  <span class="links">
                 
                 
                    
          
          
            
            
          
          
            
            
          
          
            <a href='/'>Exploratory Analysis in R</a> > <a href='/module4/'>Module 4</a> > Classification
          
        
          
        
          
        
                 
                  </span>
                </div>
                
                    <div class="progress">
    <div class="wrapper">

    </div>
</div>

                
              </div>
            </div>
            
        <div id="head-tags">
        
        </div>
        
        <div id="body-inner">
          
            <h1>
              
              Classification
            </h1>
          

        




	
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>


<p>Classification is a form of supervised learning focused on building and evaluating models for qualitative response, which we often refer to as categorical response. For example, is someone a smoker is a qualitative variable taking values <em>yes</em> and <em>no</em>. Another example would be <em>European Country</em>, which is a qualitative variable taking qualitative values: Italy, France, Spain, Great Britain, Germany, Serbia,… Here, we will study approaches for predicting qualitative responses, a <strong>process</strong> that is known as <strong>classification</strong>.</p>
<p>Predicting a qualitative response for an observation can be referred to as classifying that observation, since it assigns to a category, or class. Classification methods tend to predict the probability of each of the categories of a qualitative variable as a basis for generating the classification. In this sense they also behave like regression methods. Although many of the regression
modelling techniques can be used for classification, the way we evaluate
model performance is, needless to say, different since MSE and <span class="math inline">\(R^2\)</span> are not appropriate in the context of classification.</p>
<p>Classification problems occur often, perhaps even more so than regression problems. Classification methods have wide practical applications. In environmental studies they are used to assess flood susceptibility. They are a popular financial tool for the assessment of bankruptcy prediction. In medical studies, breast cancer is often classified according to the number of oestrogen receptors present on the tumour. These are just a few of many examples where classification methods are used to perform the prediction.</p>
<p>There are many <strong>classification methods</strong>, also known as <strong>classifiers</strong>, that can be used to predict a qualitative response. We will familiarise ourself with the two most widely-used classifiers: K-nearest neighbours and logistic regression.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>K-nearest neighbours</strong></font>
</p>
<p>K-nearest neighbours (KNN) algorithm is one of the simplest techniques used in machine learning. Because of its simplicity and low calculation time KNN is a popular classification method preferred by many. It is a simple recommendation system used by <a href="https://www.amazon.co.uk/">Amazon</a> and <a href="https://www.netflix.com/">Netflix</a> (see <a href="http://cs229.stanford.edu/proj2006/HongTsamis-KNNForNetflix.pdf">Use of KNN for the Netflix Prize</a>). It is a non-parametric method meaning that the model is made up entirely from the data given to it, instead of being based on the assumptions about the structure of the given data. In other words the KNN approach makes no assumptions about the shape of the decision boundary.</p>
<p>When implementing KNN, the first step is to transform data points into feature vectors, or their mathematical value. The algorithm then works by finding the distance between the mathematical values of these points. The most common way to find this distance is the Euclidean distance</p>
<p><span class="math display">\[
\begin{equation} 
\begin{split}
d(p, q) &amp; = d(q, p) \\
&amp; &amp; =  \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + \dotsc + (p_n - q_n)^2} \\ 
&amp; &amp;= \sqrt{\sum_{i=1}^{n}(p_i-q_i)^2}
\end{split}
\end{equation}
\]</span>
where</p>
<ul>
<li><span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are two points in Euclidean <span class="math inline">\(n\)</span>-space and</li>
<li><span class="math inline">\(q_i\)</span> and <span class="math inline">\(p_i\)</span> are Euclidean vectors, starting from the origin of the space (initial point).</li>
</ul>
<p>This can be illustrated as</p>
<p><img src="images/euclidian_distance.png?width=20pc" /></p>
<p>The figure below provides an illustrative example of the KNN approach. It consists of a small training data set of twelve red dots and twelve green squares. The goal is to make a prediction for the point labelled by the yellow star. For the choice of <span class="math inline">\(K=3\)</span> the KNN will identify the three observations that are closest to the yellow star. These neighbour observations are shaded in the circle around the yellow star. It consists of two red points and one green square, resulting in estimated probabilities of 2/3 for the red circle class and 1/3 for the green square class. As a result KNN will predict that the yellow star belongs to the red circle class.</p>
<p><img src="images/KNN.png?width=30pc" /></p>
<p>Let us assume we are dealing with a problem of classification of animals into two categories: dogs or cats. Classification needs to be conducted in respect of the two measurements:</p>
<ul>
<li>Measurement <strong>A</strong>: The length of the animal (in cm) from the tip of its nose to the back of its body, excluding the tail</li>
<li>Measurement <strong>B</strong>: The height of the animal’s leg from floor to shoulder (in cm)</li>
</ul>
<p>We have a test data with the length and height measurements of several dogs and cats. For a new set of observed measurements we need to be able to determine if they belong to a dog or a cat. That is, the unknown animal needs to be classified into one of the two groups based on the given measurements. To do this we create a 2D plane like the one above. Next, we follow the same procedure of calculating the distance of the new object from the older ones, and the closer it is to either one of the categories will determine its own category.</p>
<pre class="r"><code>animal &lt;- as.factor(c(&quot;cat&quot;, &quot;dog&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;dog&quot;, &quot;dog&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;cat&quot;, &quot;dog&quot;, &quot;unknown&quot;))
A &lt;- c(40, 34, 43, 37, 30, 39, 40, 43, 42, 37, 41, 35, 39)
B &lt;- c(24, 20, 23, 21, 26, 30, 39, 25, 27, 35, 18, 28, 20)

animals_data &lt;- data.frame(animal, A, B)

library(ggplot2)
ggplot(animals_data, aes(x = A, y = B)) +
  geom_point(aes(col = animal), shape = 20, size = 3) +
  geom_point(x = 39, y = 20, shape = 13, size = 7) +
  labs (title= &quot;Cats &amp; Dogs&quot;,
        x = &quot;length&quot;, y = &quot;height&quot;) +
  theme_minimal() +
  theme(legend.position = &quot;bottom&quot;, 
        panel.border = element_rect(fill = NA, 
                                    colour = &quot;black&quot;,
                                    size = .75),
        plot.title=element_text(hjust=0.5)) +
  scale_colour_brewer(palette = &quot;Set1&quot;) </code></pre>
<p><img src="/module4/Classification/_index.en_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>We can see that the new animal with 20cm of height and 39cm of length is in proximity with cats more than dogs. Hence, we predict that the new animal is a cat. However, in most cases we will deal with situations in which categories will be defined by many more attributes that will prevent us from simply drawing them on a scatter diagram like we did in this example.</p>
<p>The more arguments describing the categories we have greater chance of appropriate prediction there is. For example if we get two more measurements to consider:</p>
<ul>
<li>Measurement <strong>C</strong>: The width of the animal (in cm) across the shoulders, or the widest part of the animal if not the shoulders</li>
<li>Measurement <strong>D</strong>: The height of the animal (in cm) from the ground to the top of its head, including the ears if they are erect</li>
</ul>
<p>we simply use the above stated Euclidean formula to calculate the distance of the new upcoming animal from the already given animals. We take the square root of</p>
<p><span class="math display">\[\sqrt{(A_1-A_2)^2 + (B_1-B_2)^2 + (C_1-C_2)^2 + (D_1-D_2)^2}\]</span></p>
<p>where 1 is representing the already drawn data point and 2 is representing the new data point that you want to determine the category of. This calculation shown above will be used with every data point available. That is, it will run as many times as there are rows in your dataset.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>KNN summary</strong></font>
</p>
<p>When running the KNN algorithm we first need to define the value <span class="math inline">\(K\)</span>. If <span class="math inline">\(K\)</span> is 3 it will check the 3 closest neighbours in order to determine the category. If the majority of neighbours belong to a certain category from within those three nearest neighbours, then that will be chosen as the category of the upcoming object.</p>
<p>As different variables have different scaling units, like weight in kg and height in cm, they need to be normalised prior to their use in the calculation of their distance:</p>
<p><span class="math display">\[\text{standardised } x =\frac{(x-min(x))}{(min(x) — max(x))}\]</span>
This will convert the values onto a scale 0 to 1.</p>
<p>Note that this can be done with numerical variables, which is not to say that KNN cannot deal with categorical variables too. There is much more than just Euclidean distance that can be used in the KNN algorithm. There are various theoretical measures which may be much more appropriate in those cases: Jaccard similarity, Tanimoto coefficient, Sørensen–Dice coefficient and so on.</p>
<p>In a nutshell, when running the KNN algorithm, data should be normalised prior to splitting it into training and testing sets. KNN runs a formula to compute the distance between each data point and the test data. It then finds the probability of these points being similar to the test data and classifies it based on which points share the highest probabilities.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>Example in R</strong></font>
</p>
<p>When calculating distances between data points in the KNN algorithm, we must use numeric predictor variables only. However, the outcome variable for KNN classification should remain a factor variable.</p>
<p>In the following examples we will apply the KNN classification in R using two packages: the <code>class</code> and the <code>caret</code>. We will also conduct KNN classification on two data sets: one in which all predictor variables are the measured type and the other data set in which some of the predictor variables are attribute variables with a different number of levels.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>Using the <code>class</code> package and the <code>iris</code> data</strong></font>
</p>
<p>To run the KNN algorithm in R we will use the <code>knn()</code> function which is part of the <code>class</code> package. This function requires the following input information:</p>
<ol style="list-style-type: decimal">
<li>a matrix containing the predictor associated with the training data: <code>training.X</code></li>
<li>a matrix containing the predictor associated with the data for which we wish to make predictors: <code>test.X</code></li>
<li>a vector containing the class labels for the training observations: <code>train</code></li>
<li>a number of nearest neighbours to be used by the classifier: <code>K</code></li>
</ol>
<p>Let us illustrate the K Nearest Neighbours algorithm classification on the well known Iris Dataset. This famous (Fisher’s or Anderson’s) iris data set gives the measurements in centimetres of the variables’ sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.</p>
<pre class="r"><code># load iris data
suppressPackageStartupMessages(library(dplyr))
data(&quot;iris&quot;) 
glimpse(iris)</code></pre>
<pre><code>## Rows: 150
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…
## $ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…
## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…
## $ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…</code></pre>
<p>Note that apart from the <code>Species</code> category the rest of the variables are all numeric types.</p>
<p>Since classification is a type of Supervised Learning, we will would require two sets of data: <strong>training data</strong> and <strong>test data</strong>. Hence, we will divide the dataset into two subsets in the proportions of 80:20 percent. Since the <code>iris</code> dataset is sorted by <code>Species</code> by default, we will first have to mix up the data rows and then take the subset:</p>
<ul>
<li><code>iris.train</code>: training subset</li>
<li><code>iris.test</code>: testing subset</li>
</ul>
<pre class="r"><code>set.seed(123) # required to reproduce the results
rnums&lt;- sample(rep(1:150)) # randomly generate numbers from 1 to 150; 150 - number of rows 
iris&lt;- iris[rnums, ] # jumbling up &quot;iris&quot; dataset by randomising 
#head(iris, n=10)
DT::datatable(iris)</code></pre>
<div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[["14","50","118","43","150","148","90","91","143","92","137","99","72","26","7","78","81","147","103","117","76","32","106","109","136","9","41","74","23","27","60","53","126","119","121","96","38","89","34","93","69","138","130","63","13","82","97","142","25","114","21","79","124","47","144","120","16","6","127","86","132","39","31","134","149","112","4","128","110","102","52","22","129","87","35","40","30","12","88","123","64","146","67","122","37","8","51","10","115","42","44","85","107","139","73","20","46","17","54","108","75","80","71","15","24","68","133","145","29","104","45","140","101","135","95","116","5","111","94","49","100","62","11","84","56","105","19","36","57","125","61","2","66","83","3","65","70","55","77","1","98","113","28","33","18","48","59","131","58","141"],[4.3,5,7.7,4.4,5.9,6.5,5.5,5.5,5.8,6.1,6.3,5.1,6.1,5,4.6,6.7,5.5,6.3,7.1,6.5,6.6,5.4,7.6,6.7,7.7,4.4,5,6.1,4.6,5,5.2,6.9,7.2,7.7,6.9,5.7,4.9,5.6,5.5,5.8,6.2,6.4,7.2,6,4.8,5.5,5.7,6.9,4.8,5.7,5.4,6,6.3,5.1,6.8,6,5.7,5.4,6.2,6,7.9,4.4,4.8,6.3,6.2,6.4,4.6,6.1,7.2,5.8,6.4,5.1,6.4,6.7,4.9,5.1,4.7,4.8,6.3,7.7,6.1,6.7,5.6,5.6,5.5,5,7,4.9,5.8,4.5,5,5.4,4.9,6,6.3,5.1,4.8,5.4,5.5,7.3,6.4,5.7,5.9,5.8,5.1,5.8,6.4,6.7,5.2,6.3,5.1,6.9,6.3,6.1,5.6,6.4,5,6.5,5,5.3,5.7,5.9,5.4,6,5.7,6.5,5.7,5,6.3,6.7,5,4.9,6.7,5.8,4.7,5.6,5.6,6.5,6.8,5.1,6.2,6.8,5.2,5.2,5.1,4.6,6.6,7.4,4.9,6.7],[3,3.3,3.8,3.2,3,3,2.5,2.6,2.7,3,3.4,2.5,2.8,3,3.4,3,2.4,2.5,3,3,3,3.4,3,2.5,3,2.9,3.5,2.8,3.6,3.4,2.7,3.1,3.2,2.6,3.2,3,3.6,3,4.2,2.6,2.2,3.1,3,2.2,3,2.4,2.9,3.1,3.4,2.5,3.4,2.9,2.7,3.8,3.2,2.2,4.4,3.9,2.8,3.4,3.8,3,3.1,2.8,3.4,2.7,3.1,3,3.6,2.7,3.2,3.7,2.8,3.1,3.1,3.4,3.2,3.4,2.3,2.8,2.9,3,3,2.8,3.5,3.4,3.2,3.1,2.8,2.3,3.5,3,2.5,3,2.5,3.8,3,3.9,2.3,2.9,2.9,2.6,3.2,4,3.3,2.7,2.8,3.3,3.4,2.9,3.8,3.1,3.3,2.6,2.7,3.2,3.6,3.2,2.3,3.7,2.8,3,3.7,2.7,2.8,3,3.8,3.2,3.3,3.3,2,3,3.1,2.7,3.2,2.9,2.5,2.8,2.8,3.5,2.9,3,3.5,4.1,3.5,3.2,2.9,2.8,2.4,3.1],[1.1,1.4,6.7,1.3,5.1,5.2,4,4.4,5.1,4.6,5.6,3,4,1.6,1.4,5,3.8,5,5.9,5.5,4.4,1.5,6.6,5.8,6.1,1.4,1.3,4.7,1,1.6,3.9,4.9,6,6.9,5.7,4.2,1.4,4.1,1.4,4,4.5,5.5,5.8,4,1.4,3.7,4.2,5.1,1.9,5,1.7,4.5,4.9,1.6,5.9,5,1.5,1.7,4.8,4.5,6.4,1.3,1.6,5.1,5.4,5.3,1.5,4.9,6.1,5.1,4.5,1.5,5.6,4.7,1.5,1.5,1.6,1.6,4.4,6.7,4.7,5.2,4.5,4.9,1.3,1.5,4.7,1.5,5.1,1.3,1.6,4.5,4.5,4.8,4.9,1.5,1.4,1.3,4,6.3,4.3,3.5,4.8,1.2,1.7,4.1,5.6,5.7,1.4,5.6,1.9,5.4,6,5.6,4.2,5.3,1.4,5.1,3.3,1.5,4.1,4.2,1.5,5.1,4.5,5.8,1.7,1.2,4.7,5.7,3.5,1.4,4.4,3.9,1.3,3.6,3.9,4.6,4.8,1.4,4.3,5.5,1.5,1.5,1.4,1.4,4.6,6.1,3.3,5.6],[0.1,0.2,2.2,0.2,1.8,2,1.3,1.2,1.9,1.4,2.4,1.1,1.3,0.2,0.3,1.7,1.1,1.9,2.1,1.8,1.4,0.4,2.1,1.8,2.3,0.2,0.3,1.2,0.2,0.4,1.4,1.5,1.8,2.3,2.3,1.2,0.1,1.3,0.2,1.2,1.5,1.8,1.6,1,0.1,1,1.3,2.3,0.2,2,0.2,1.5,1.8,0.2,2.3,1.5,0.4,0.4,1.8,1.6,2,0.2,0.2,1.5,2.3,1.9,0.2,1.8,2.5,1.9,1.5,0.4,2.1,1.5,0.2,0.2,0.2,0.2,1.3,2,1.4,2.3,1.5,2,0.2,0.2,1.4,0.1,2.4,0.3,0.6,1.5,1.7,1.8,1.5,0.3,0.3,0.4,1.3,1.8,1.3,1,1.8,0.2,0.5,1,2.2,2.5,0.2,1.8,0.4,2.1,2.5,1.4,1.3,2.3,0.2,2,1,0.2,1.3,1.5,0.2,1.6,1.3,2.2,0.3,0.2,1.6,2.1,1,0.2,1.4,1.2,0.2,1.3,1.1,1.5,1.4,0.2,1.3,2.1,0.2,0.1,0.3,0.2,1.3,1.9,1,2.4],["setosa","setosa","virginica","setosa","virginica","virginica","versicolor","versicolor","virginica","versicolor","virginica","versicolor","versicolor","setosa","setosa","versicolor","versicolor","virginica","virginica","virginica","versicolor","setosa","virginica","virginica","virginica","setosa","setosa","versicolor","setosa","setosa","versicolor","versicolor","virginica","virginica","virginica","versicolor","setosa","versicolor","setosa","versicolor","versicolor","virginica","virginica","versicolor","setosa","versicolor","versicolor","virginica","setosa","virginica","setosa","versicolor","virginica","setosa","virginica","virginica","setosa","setosa","virginica","versicolor","virginica","setosa","setosa","virginica","virginica","virginica","setosa","virginica","virginica","virginica","versicolor","setosa","virginica","versicolor","setosa","setosa","setosa","setosa","versicolor","virginica","versicolor","virginica","versicolor","virginica","setosa","setosa","versicolor","setosa","virginica","setosa","setosa","versicolor","virginica","virginica","versicolor","setosa","setosa","setosa","versicolor","virginica","versicolor","versicolor","versicolor","setosa","setosa","versicolor","virginica","virginica","setosa","virginica","setosa","virginica","virginica","virginica","versicolor","virginica","setosa","virginica","versicolor","setosa","versicolor","versicolor","setosa","versicolor","versicolor","virginica","setosa","setosa","versicolor","virginica","versicolor","setosa","versicolor","versicolor","setosa","versicolor","versicolor","versicolor","versicolor","setosa","versicolor","virginica","setosa","setosa","setosa","setosa","versicolor","virginica","versicolor","virginica"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Sepal.Length<\/th>\n      <th>Sepal.Width<\/th>\n      <th>Petal.Length<\/th>\n      <th>Petal.Width<\/th>\n      <th>Species<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Remember that data should be normalised prior to splitting it into training and testing sets. Therefore, as our next step we will normalise data, scaling it to a z-score metric to have values between 0 and 1, after which we will split it into two subsets: training and testing.</p>
<pre class="r"><code># normalise data
normalise &lt;- function(x){
  return ((x-min(x)) / (max(x)-min(x)))
}

iris_norm &lt;- as.data.frame(lapply(iris[,c(1,2,3,4)], normalise))
#head(iris_norm, n=10)
DT::datatable(iris_norm)</code></pre>
<div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150"],[0,0.194444444444444,0.944444444444444,0.0277777777777779,0.444444444444445,0.611111111111111,0.333333333333333,0.333333333333333,0.416666666666667,0.5,0.555555555555555,0.222222222222222,0.5,0.194444444444444,0.0833333333333333,0.666666666666667,0.333333333333333,0.555555555555555,0.777777777777778,0.611111111111111,0.638888888888889,0.305555555555556,0.916666666666667,0.666666666666667,0.944444444444444,0.0277777777777779,0.194444444444444,0.5,0.0833333333333333,0.194444444444444,0.25,0.722222222222222,0.805555555555556,0.944444444444444,0.722222222222222,0.388888888888889,0.166666666666667,0.361111111111111,0.333333333333333,0.416666666666667,0.527777777777778,0.583333333333333,0.805555555555556,0.472222222222222,0.138888888888889,0.333333333333333,0.388888888888889,0.722222222222222,0.138888888888889,0.388888888888889,0.305555555555556,0.472222222222222,0.555555555555555,0.222222222222222,0.694444444444444,0.472222222222222,0.388888888888889,0.305555555555556,0.527777777777778,0.472222222222222,1,0.0277777777777779,0.138888888888889,0.555555555555555,0.527777777777778,0.583333333333333,0.0833333333333333,0.5,0.805555555555556,0.416666666666667,0.583333333333333,0.222222222222222,0.583333333333333,0.666666666666667,0.166666666666667,0.222222222222222,0.111111111111111,0.138888888888889,0.555555555555555,0.944444444444444,0.5,0.666666666666667,0.361111111111111,0.361111111111111,0.333333333333333,0.194444444444444,0.75,0.166666666666667,0.416666666666667,0.0555555555555556,0.194444444444444,0.305555555555556,0.166666666666667,0.472222222222222,0.555555555555555,0.222222222222222,0.138888888888889,0.305555555555556,0.333333333333333,0.833333333333333,0.583333333333333,0.388888888888889,0.444444444444445,0.416666666666667,0.222222222222222,0.416666666666667,0.583333333333333,0.666666666666667,0.25,0.555555555555555,0.222222222222222,0.722222222222222,0.555555555555555,0.5,0.361111111111111,0.583333333333333,0.194444444444444,0.611111111111111,0.194444444444444,0.277777777777778,0.388888888888889,0.444444444444445,0.305555555555556,0.472222222222222,0.388888888888889,0.611111111111111,0.388888888888889,0.194444444444444,0.555555555555555,0.666666666666667,0.194444444444444,0.166666666666667,0.666666666666667,0.416666666666667,0.111111111111111,0.361111111111111,0.361111111111111,0.611111111111111,0.694444444444444,0.222222222222222,0.527777777777778,0.694444444444444,0.25,0.25,0.222222222222222,0.0833333333333333,0.638888888888889,0.861111111111111,0.166666666666667,0.666666666666667],[0.416666666666667,0.541666666666667,0.75,0.5,0.416666666666667,0.416666666666667,0.208333333333333,0.25,0.291666666666667,0.416666666666667,0.583333333333333,0.208333333333333,0.333333333333333,0.416666666666667,0.583333333333333,0.416666666666667,0.166666666666667,0.208333333333333,0.416666666666667,0.416666666666667,0.416666666666667,0.583333333333333,0.416666666666667,0.208333333333333,0.416666666666667,0.375,0.625,0.333333333333333,0.666666666666667,0.583333333333333,0.291666666666667,0.458333333333333,0.5,0.25,0.5,0.416666666666667,0.666666666666667,0.416666666666667,0.916666666666667,0.25,0.0833333333333334,0.458333333333333,0.416666666666667,0.0833333333333334,0.416666666666667,0.166666666666667,0.375,0.458333333333333,0.583333333333333,0.208333333333333,0.583333333333333,0.375,0.291666666666667,0.75,0.5,0.0833333333333334,1,0.791666666666667,0.333333333333333,0.583333333333333,0.75,0.416666666666667,0.458333333333333,0.333333333333333,0.583333333333333,0.291666666666667,0.458333333333333,0.416666666666667,0.666666666666667,0.291666666666667,0.5,0.708333333333333,0.333333333333333,0.458333333333333,0.458333333333333,0.583333333333333,0.5,0.583333333333333,0.125,0.333333333333333,0.375,0.416666666666667,0.416666666666667,0.333333333333333,0.625,0.583333333333333,0.5,0.458333333333333,0.333333333333333,0.125,0.625,0.416666666666667,0.208333333333333,0.416666666666667,0.208333333333333,0.75,0.416666666666667,0.791666666666667,0.125,0.375,0.375,0.25,0.5,0.833333333333333,0.541666666666667,0.291666666666667,0.333333333333333,0.541666666666667,0.583333333333333,0.375,0.75,0.458333333333333,0.541666666666667,0.25,0.291666666666667,0.5,0.666666666666667,0.5,0.125,0.708333333333333,0.333333333333333,0.416666666666667,0.708333333333333,0.291666666666667,0.333333333333333,0.416666666666667,0.75,0.5,0.541666666666667,0.541666666666667,0,0.416666666666667,0.458333333333333,0.291666666666667,0.5,0.375,0.208333333333333,0.333333333333333,0.333333333333333,0.625,0.375,0.416666666666667,0.625,0.875,0.625,0.5,0.375,0.333333333333333,0.166666666666667,0.458333333333333],[0.0169491525423729,0.0677966101694915,0.966101694915254,0.0508474576271186,0.694915254237288,0.711864406779661,0.508474576271186,0.576271186440678,0.694915254237288,0.610169491525424,0.779661016949152,0.338983050847458,0.508474576271186,0.101694915254237,0.0677966101694915,0.677966101694915,0.474576271186441,0.677966101694915,0.830508474576271,0.76271186440678,0.576271186440678,0.0847457627118644,0.949152542372881,0.813559322033898,0.864406779661017,0.0677966101694915,0.0508474576271186,0.627118644067797,0,0.101694915254237,0.491525423728814,0.661016949152542,0.847457627118644,1,0.796610169491525,0.542372881355932,0.0677966101694915,0.525423728813559,0.0677966101694915,0.508474576271186,0.593220338983051,0.76271186440678,0.813559322033898,0.508474576271186,0.0677966101694915,0.457627118644068,0.542372881355932,0.694915254237288,0.152542372881356,0.677966101694915,0.11864406779661,0.593220338983051,0.661016949152542,0.101694915254237,0.830508474576271,0.677966101694915,0.0847457627118644,0.11864406779661,0.644067796610169,0.593220338983051,0.915254237288136,0.0508474576271186,0.101694915254237,0.694915254237288,0.745762711864407,0.728813559322034,0.0847457627118644,0.661016949152542,0.864406779661017,0.694915254237288,0.593220338983051,0.0847457627118644,0.779661016949152,0.627118644067797,0.0847457627118644,0.0847457627118644,0.101694915254237,0.101694915254237,0.576271186440678,0.966101694915254,0.627118644067797,0.711864406779661,0.593220338983051,0.661016949152542,0.0508474576271186,0.0847457627118644,0.627118644067797,0.0847457627118644,0.694915254237288,0.0508474576271186,0.101694915254237,0.593220338983051,0.593220338983051,0.644067796610169,0.661016949152542,0.0847457627118644,0.0677966101694915,0.0508474576271186,0.508474576271186,0.898305084745763,0.559322033898305,0.423728813559322,0.644067796610169,0.0338983050847458,0.11864406779661,0.525423728813559,0.779661016949152,0.796610169491525,0.0677966101694915,0.779661016949152,0.152542372881356,0.745762711864407,0.847457627118644,0.779661016949152,0.542372881355932,0.728813559322034,0.0677966101694915,0.694915254237288,0.389830508474576,0.0847457627118644,0.525423728813559,0.542372881355932,0.0847457627118644,0.694915254237288,0.593220338983051,0.813559322033898,0.11864406779661,0.0338983050847458,0.627118644067797,0.796610169491525,0.423728813559322,0.0677966101694915,0.576271186440678,0.491525423728814,0.0508474576271186,0.440677966101695,0.491525423728814,0.610169491525424,0.644067796610169,0.0677966101694915,0.559322033898305,0.76271186440678,0.0847457627118644,0.0847457627118644,0.0677966101694915,0.0677966101694915,0.610169491525424,0.864406779661017,0.389830508474576,0.779661016949152],[0,0.0416666666666667,0.875,0.0416666666666667,0.708333333333333,0.791666666666667,0.5,0.458333333333333,0.75,0.541666666666667,0.958333333333333,0.416666666666667,0.5,0.0416666666666667,0.0833333333333333,0.666666666666667,0.416666666666667,0.75,0.833333333333333,0.708333333333333,0.541666666666667,0.125,0.833333333333333,0.708333333333333,0.916666666666667,0.0416666666666667,0.0833333333333333,0.458333333333333,0.0416666666666667,0.125,0.541666666666667,0.583333333333333,0.708333333333333,0.916666666666667,0.916666666666667,0.458333333333333,0,0.5,0.0416666666666667,0.458333333333333,0.583333333333333,0.708333333333333,0.625,0.375,0,0.375,0.5,0.916666666666667,0.0416666666666667,0.791666666666667,0.0416666666666667,0.583333333333333,0.708333333333333,0.0416666666666667,0.916666666666667,0.583333333333333,0.125,0.125,0.708333333333333,0.625,0.791666666666667,0.0416666666666667,0.0416666666666667,0.583333333333333,0.916666666666667,0.75,0.0416666666666667,0.708333333333333,1,0.75,0.583333333333333,0.125,0.833333333333333,0.583333333333333,0.0416666666666667,0.0416666666666667,0.0416666666666667,0.0416666666666667,0.5,0.791666666666667,0.541666666666667,0.916666666666667,0.583333333333333,0.791666666666667,0.0416666666666667,0.0416666666666667,0.541666666666667,0,0.958333333333333,0.0833333333333333,0.208333333333333,0.583333333333333,0.666666666666667,0.708333333333333,0.583333333333333,0.0833333333333333,0.0833333333333333,0.125,0.5,0.708333333333333,0.5,0.375,0.708333333333333,0.0416666666666667,0.166666666666667,0.375,0.875,1,0.0416666666666667,0.708333333333333,0.125,0.833333333333333,1,0.541666666666667,0.5,0.916666666666667,0.0416666666666667,0.791666666666667,0.375,0.0416666666666667,0.5,0.583333333333333,0.0416666666666667,0.625,0.5,0.875,0.0833333333333333,0.0416666666666667,0.625,0.833333333333333,0.375,0.0416666666666667,0.541666666666667,0.458333333333333,0.0416666666666667,0.5,0.416666666666667,0.583333333333333,0.541666666666667,0.0416666666666667,0.5,0.833333333333333,0.0416666666666667,0,0.0833333333333333,0.0416666666666667,0.5,0.75,0.375,0.958333333333333]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Sepal.Length<\/th>\n      <th>Sepal.Width<\/th>\n      <th>Petal.Length<\/th>\n      <th>Petal.Width<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<pre class="r"><code># split data
iris_train &lt;- iris_norm[1:120, ]
iris_train_target &lt;- iris[1:120, 5]
iris_test &lt;- iris_norm[121:150, ]
iris_test_target &lt;- iris[121:150, 5]
summary(iris_norm)</code></pre>
<pre><code>##   Sepal.Length     Sepal.Width      Petal.Length     Petal.Width     
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.00000  
##  1st Qu.:0.2222   1st Qu.:0.3333   1st Qu.:0.1017   1st Qu.:0.08333  
##  Median :0.4167   Median :0.4167   Median :0.5678   Median :0.50000  
##  Mean   :0.4287   Mean   :0.4406   Mean   :0.4675   Mean   :0.45806  
##  3rd Qu.:0.5833   3rd Qu.:0.5417   3rd Qu.:0.6949   3rd Qu.:0.70833  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.00000</code></pre>
<p>Once we have prepared our data for the application of the KNN classification we can run the algorithm. We can also obtain predicted probabilities, given test predictors by setting the probability argument as true: <code>prob = TRUE</code>.</p>
<pre class="r"><code># If you don&#39;t have class installed yet, uncomment and run the line below
#install.packages(&quot;class&quot;)
library(class)
model_knn &lt;- knn(train = iris_train, 
                 test = iris_test, 
                 cl = iris_train_target, 
                 k = 10,
                 prob  = TRUE)
model_knn</code></pre>
<pre><code>##  [1] versicolor versicolor setosa     virginica  versicolor virginica 
##  [7] setosa     setosa     versicolor virginica  versicolor setosa    
## [13] versicolor versicolor setosa     versicolor versicolor versicolor
## [19] versicolor setosa     versicolor virginica  setosa     setosa    
## [25] setosa     setosa     versicolor virginica  versicolor virginica 
## attr(,&quot;prob&quot;)
##  [1] 1.0 0.9 1.0 0.7 1.0 1.0 1.0 1.0 0.7 1.0 1.0 1.0 0.9 1.0 1.0 1.0 1.0 0.8 0.9
## [20] 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 1.0 1.0 1.0
## Levels: setosa versicolor virginica</code></pre>
<pre class="r"><code>attributes(model_knn)$prob</code></pre>
<pre><code>##  [1] 1.0 0.9 1.0 0.7 1.0 1.0 1.0 1.0 0.7 1.0 1.0 1.0 0.9 1.0 1.0 1.0 1.0 0.8 0.9
## [20] 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 1.0 1.0 1.0</code></pre>
<p>Unfortunately, this only returns the predicted probability of the most common class. In the binary case, this would be sufficient to recover all probabilities, however, for multi-class problems, we cannot recover each of the probabilities of interest. This will simply be a minor annoyance for now, which we will fix when we introduce the <code>caret</code> package for model training.</p>
<p>A common method for describing the performance of a classification model is the <strong><em>confusion matrix</em></strong>. So, to check how well the model has performed we will construct the confusion matrix, which is a table that is used to show the number of correct and incorrect predictions on a classification problem when the real values of the <em>test data</em> are known. For a two class problem it is of the format</p>
<p><img src="images/confusion_matrix.png?width=30pc" /></p>
<p>The <code>TRUE</code> values are the number of correct predictions made.</p>
<pre class="r"><code>how_well &lt;- data.frame(model_knn, iris_test_target) %&gt;% 
  mutate(result = model_knn == iris_test_target)

how_well</code></pre>
<pre><code>##     model_knn iris_test_target result
## 1  versicolor       versicolor   TRUE
## 2  versicolor       versicolor   TRUE
## 3      setosa           setosa   TRUE
## 4   virginica       versicolor  FALSE
## 5  versicolor       versicolor   TRUE
## 6   virginica        virginica   TRUE
## 7      setosa           setosa   TRUE
## 8      setosa           setosa   TRUE
## 9  versicolor       versicolor   TRUE
## 10  virginica        virginica   TRUE
## 11 versicolor       versicolor   TRUE
## 12     setosa           setosa   TRUE
## 13 versicolor       versicolor   TRUE
## 14 versicolor       versicolor   TRUE
## 15     setosa           setosa   TRUE
## 16 versicolor       versicolor   TRUE
## 17 versicolor       versicolor   TRUE
## 18 versicolor       versicolor   TRUE
## 19 versicolor       versicolor   TRUE
## 20     setosa           setosa   TRUE
## 21 versicolor       versicolor   TRUE
## 22  virginica        virginica   TRUE
## 23     setosa           setosa   TRUE
## 24     setosa           setosa   TRUE
## 25     setosa           setosa   TRUE
## 26     setosa           setosa   TRUE
## 27 versicolor       versicolor   TRUE
## 28  virginica        virginica   TRUE
## 29 versicolor       versicolor   TRUE
## 30  virginica        virginica   TRUE</code></pre>
<pre class="r"><code>confusion_matrix &lt;- table(iris_test_target, model_knn)

summary(model_knn)</code></pre>
<pre><code>##     setosa versicolor  virginica 
##         10         14          6</code></pre>
<pre class="r"><code>confusion_matrix</code></pre>
<pre><code>##                 model_knn
## iris_test_target setosa versicolor virginica
##       setosa         10          0         0
##       versicolor      0         14         1
##       virginica       0          0         5</code></pre>
<p>The values on the diagonal shows the number of correctly classified instances for each category. In our example it is showing that the KNN algorithm has correctly classified 29 instances out of a total 30 data points in the test data. The values not on the diagonal imply that they have been incorrectly classified. In our case it is only 1 observation, giving an accuracy of 97%.</p>
<p>The simplest metric for model classification is the overall <strong><em>accuracy rate</em></strong>. To evaluate predicted classes, we are going to create a function that can calculate the accuracy of the KNN algorithm.</p>
<pre class="r"><code># calculate percentage accuracy
accuracy &lt;- function(x){
  sum(diag(x) / (sum(rowSums(x)))) * 100
}
accuracy(confusion_matrix)</code></pre>
<pre><code>## [1] 96.66667</code></pre>
<p>Although the overall accuracy rate might be easy to compute and to interpret, it makes no distinction about the type of errors being made. Choosing the correct level of flexibility is critical to the success of any machine learning method. For the KNN method this can be adjusted by trying different values of <code>K</code>, but the bias-variance trade off can make this a difficult task.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>Using the <code>caret</code> package and the <code>penguins</code> data</strong></font>
</p>
<p>The R’s <a href="https://cran.r-project.org/web/packages/caret/caret.pdf"><code>caret</code></a> package (<strong>c</strong>lassification <strong>a</strong>nd <strong>re</strong>gression <strong>t</strong>raining) holds many functions that help to build predictive models. It holds tools for data splitting, pre-processing, feature selection, tuning and supervised – unsupervised learning algorithms, etc. The <code>caret</code> package provides direct access to various functions for training a model with a machine learning algorithm for KNN classification. But, before we can use <code>caret</code> to perform various tasks to carry out the necessary machine learning work, we need to familiarise ourselves with our data.</p>
<p>The <code>penguins</code> data set is available from the <code>palmerpenguins</code> package. It includes 6 measurements of 344 penguin species and information about the year during which data was collected. We will start by loading the R packages we will need for this exercise and reading the <code>penguins</code> data set.</p>
<pre class="r"><code># If you don&#39;t have tidyr, caret, gmodels, psych, palmerpenguins and pscl installed yet, uncomment and run the lines below
#install.packages(&quot;tidyr&quot;)
#install.packages(&quot;caret&quot;)
#install.packages(&quot;gmodels&quot;)
#install.packages(&quot;psych&quot;)
#install.packages(&quot;palmerpenguins&quot;)
#install.packages(&quot;pscl&quot;) # pR2()
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(gmodels)) # CrossTable()
suppressPackageStartupMessages(library(psych)) # dummy.code()
library(palmerpenguins)
library(ggplot2)

penguins &lt;- read.csv(path_to_file(&quot;penguins.csv&quot;))
glimpse(penguins)</code></pre>
<pre><code>## Rows: 344
## Columns: 8
## $ species           &lt;chr&gt; &quot;Adelie&quot;, &quot;Adelie&quot;, &quot;Adelie&quot;, &quot;Adelie&quot;, &quot;Adelie&quot;, &quot;A…
## $ island            &lt;chr&gt; &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, …
## $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …
## $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …
## $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…
## $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …
## $ sex               &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, NA, &quot;female&quot;, &quot;male&quot;, &quot;f…
## $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…</code></pre>
<ul>
<li><code>species</code> a factor denoting penguin species (Adélie, Chinstrap and Gentoo)</li>
<li><code>island</code> a factor denoting an island in the Palmer Archipelago, Antarctica (Biscoe, Dream or Torgersen)</li>
<li><code>bill_length_mm</code> a number denoting bill length (millimetres)</li>
<li><code>bill_depth_mm</code> a number denoting bill depth (millimetres)</li>
<li><code>flipper_length_mm</code> an integer denoting flipper length (millimetres)</li>
<li><code>body_mass_g</code> an integer denoting body mass (grams)</li>
<li><code>sex</code> a factor denoting penguin sex (female, male)</li>
<li><code>year</code> an integer denoting the study year (2007, 2008 or 2009)</li>
</ul>
<p>For k-NN classification, we are going to predict the categorical variable <code>species</code> using all the other variables within the data set. This data set has both measured and attribute variables as the predictors. As <code>year</code> is a variable of the recording we will not consider it in our modelling and therefore we will remove it from the data set we will work with. ML algorithms are sensitive to missing values. For the aim of this analysis, instead of constructing imputation for the missing values we will simply drop them from the data set.</p>
<pre class="r"><code>pingo &lt;- penguins[1:7]

pingo %&gt;%
  summarise_all(funs(sum(is.na(.)))) %&gt;%
  pivot_longer(cols = 1:7, names_to = &#39;columns&#39;, values_to = &#39;NA_count&#39;) %&gt;%
  arrange(desc(NA_count)) %&gt;%
  ggplot(aes(y = columns, x = NA_count)) + geom_col(fill = &#39;deepskyblue4&#39;) +
  geom_label(aes(label = NA_count)) +
  theme_minimal() +
  labs(title = &#39;Penguins - NA Count&#39;,
       caption = &quot;Visualisation: https://www.r-bloggers.com/2020/06/penguins-dataset-overview-iris-alternative-in-r&quot;,
       y = &quot;variables&quot;) +
  theme(plot.title = element_text(hjust = 0.5))</code></pre>
<p><img src="/module4/Classification/_index.en_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>data_class &lt;- pingo %&gt;% 
  drop_na()</code></pre>
<p>Next, we need to prepare data further for our k-NN classification. First, we will put our outcome variable <code>species</code> into its own object and remove it from the data set.</p>
<pre class="r"><code>species_outcome &lt;- data_class %&gt;% select(species)

# remove original variable from the data set
data_class &lt;- data_class %&gt;% select(-species)

glimpse(data_class)</code></pre>
<pre><code>## Rows: 333
## Columns: 6
## $ island            &lt;chr&gt; &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, …
## $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…
## $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…
## $ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…
## $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…
## $ sex               &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;femal…</code></pre>
<p>We see that <code>bill_length_mm</code>, <code>bill_depth_mm</code>, <code>flipper_length_mm</code> and <code>body_mass_g</code> are measured variables of which the features are measured on different metrics, which means they have to be scaled. To do this we are going to use the <code>scale</code> function, which means we are scaling to a z-score metric.</p>
<pre class="r"><code>data_class[, c(&quot;bill_length_mm&quot;,
               &quot;bill_depth_mm&quot;, 
               &quot;flipper_length_mm&quot;, 
               &quot;body_mass_g&quot;)] &lt;- scale(data_class[, c(&quot;bill_length_mm&quot;, 
                                                      &quot;bill_depth_mm&quot;, 
                                                      &quot;flipper_length_mm&quot;, 
                                                      &quot;body_mass_g&quot;)])
glimpse(data_class)</code></pre>
<pre><code>## Rows: 333
## Columns: 6
## $ island            &lt;chr&gt; &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, &quot;Torgersen&quot;, …
## $ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.8215515, -0.6752636, -1.3335592, -0.8…
## $ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.11940428, 0.42409105, 1.08424573, 1.74…
## $ flipper_length_mm &lt;dbl&gt; -1.4246077, -1.0678666, -0.4257325, -0.5684290, -0.7…
## $ body_mass_g       &lt;dbl&gt; -0.567620576, -0.505525421, -1.188572125, -0.9401915…
## $ sex               &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;femal…</code></pre>
<pre class="r"><code>head(data_class)</code></pre>
<pre><code>##      island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g    sex
## 1 Torgersen     -0.8946955     0.7795590        -1.4246077  -0.5676206   male
## 2 Torgersen     -0.8215515     0.1194043        -1.0678666  -0.5055254 female
## 3 Torgersen     -0.6752636     0.4240910        -0.4257325  -1.1885721 female
## 4 Torgersen     -1.3335592     1.0842457        -0.5684290  -0.9401915 female
## 5 Torgersen     -0.8581235     1.7444004        -0.7824736  -0.6918109   male
## 6 Torgersen     -0.9312674     0.3225288        -1.4246077  -0.7228585 female</code></pre>
<p>As our next step in preparing data for the application of the KNN algorithm we need to dummy code attribute variables. From data description we know that the variable <code>island</code> is an attribute variable that has three levels and <code>sex</code> is an attribute variable with two levels.</p>
<p>We will first dummy code the attribute variable <code>sex</code> that has only two levels and then the one with more than two levels using the <code>dummy.code()</code> function from the <a href="https://cran.r-project.org/web/packages/psych/psych.pdf"><code>psych</code> package</a>.</p>
<pre class="r"><code>data_class$sex &lt;- dummy.code(data_class$sex)</code></pre>
<p>For <code>island</code>, the variable <code>dummy.code()</code> function will create three new variables coded with 0 and 1, which will then need to be combined with the original data set. As the final preparation, since we have created a single variable for each island category we will now remove the original <code>island</code> variable from our data.</p>
<pre class="r"><code>island &lt;- as.data.frame(dummy.code(data_class$island))
data_class &lt;- cbind(data_class, island)
data_class &lt;- data_class %&gt;% select(-island)

glimpse(data_class)</code></pre>
<pre><code>## Rows: 333
## Columns: 8
## $ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.8215515, -0.6752636, -1.3335592, -0.8…
## $ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.11940428, 0.42409105, 1.08424573, 1.74…
## $ flipper_length_mm &lt;dbl&gt; -1.4246077, -1.0678666, -0.4257325, -0.5684290, -0.7…
## $ body_mass_g       &lt;dbl&gt; -0.567620576, -0.505525421, -1.188572125, -0.9401915…
## $ sex               &lt;dbl[,2]&gt; &lt;matrix[26 x 2]&gt;
## $ Biscoe            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …
## $ Dream             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ Torgersen         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0…</code></pre>
<pre class="r"><code>print.data.frame(head(data_class, ))</code></pre>
<pre><code>##   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex.male
## 1     -0.8946955     0.7795590        -1.4246077  -0.5676206        1
## 2     -0.8215515     0.1194043        -1.0678666  -0.5055254        0
## 3     -0.6752636     0.4240910        -0.4257325  -1.1885721        0
## 4     -1.3335592     1.0842457        -0.5684290  -0.9401915        0
## 5     -0.8581235     1.7444004        -0.7824736  -0.6918109        1
## 6     -0.9312674     0.3225288        -1.4246077  -0.7228585        0
##   sex.female Biscoe Dream Torgersen
## 1          0      0     0         1
## 2          1      0     0         1
## 3          1      0     0         1
## 4          1      0     0         1
## 5          0      0     0         1
## 6          1      0     0         1</code></pre>
<p>Once we have prepared data, we can run the KNN algorithm. First, we will split the data into training and test sets. We partition 80% of the data into the training set and the remaining 20% into the test set.</p>
<pre class="r"><code>set.seed(1) # set the seed to make the partition reproducible

# 80% of the sample size
smp_size &lt;- floor(0.80 * nrow(data_class))

train_ind &lt;- sample(seq_len(nrow(data_class)), size = smp_size)

# creating test and training sets that contain all of the predictors
class_pred_train &lt;- data_class[train_ind, ]
class_pred_test &lt;- data_class[-train_ind, ]</code></pre>
<p>We will also split the response variable into training and test sets using the same partition as above.</p>
<pre class="r"><code>species_outcome_train &lt;- species_outcome[train_ind, ]
species_outcome_test &lt;- species_outcome[-train_ind, ]</code></pre>
<p>We will start by using the <code>knn()</code> classification function available from the <code>class</code> package. There are several rules of thumb we can use when choosing the number of <code>k</code>. One that we will adopt is the square root of the number of observations in the training set, which in our case is 266. So, in this case, we select 16 as the number of neighbours.</p>
<pre class="r"><code>pingo_knn &lt;- knn(train = class_pred_train, test = class_pred_test, cl = species_outcome_train, k=16)


# put &quot;species_outcome_test&quot; in a data frame
species_outcome_test &lt;- data.frame(species_outcome_test)

# merge &quot;pingo_knn&quot; and &quot;species_outcome_test&quot; 
class_comparison &lt;- data.frame(pingo_knn, species_outcome_test)

# specify column names for &quot;class_comparison&quot;
names(class_comparison) &lt;- c(&quot;predicted_species&quot;, &quot;observed_species&quot;)

# inspect &quot;class_comparison&quot; 
DT::datatable(class_comparison)</code></pre>
<div id="htmlwidget-3" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-3">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67"],["Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Adelie","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap"],["Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Adelie","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Gentoo","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap","Chinstrap"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>predicted_species<\/th>\n      <th>observed_species<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>
<p>To evaluate the model, this time we will construct the confusion matrix by using the <code>CrossTable()</code> function available from the <a href="https://cran.r-project.org/web/packages/gmodels/gmodels.pdf"><code>gmodels</code></a> package.</p>
<pre class="r"><code>CrossTable(x = class_comparison$observed_species, y = class_comparison$predicted_species, prop.chisq=FALSE, prop.c = FALSE, prop.r = FALSE, prop.t = FALSE)</code></pre>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## |-------------------------|
## 
##  
## Total Observations in Table:  67 
## 
##  
##                                   | class_comparison$predicted_species 
## class_comparison$observed_species |    Adelie | Chinstrap |    Gentoo | Row Total | 
## ----------------------------------|-----------|-----------|-----------|-----------|
##                            Adelie |        31 |         0 |         0 |        31 | 
## ----------------------------------|-----------|-----------|-----------|-----------|
##                         Chinstrap |         1 |        10 |         0 |        11 | 
## ----------------------------------|-----------|-----------|-----------|-----------|
##                            Gentoo |         0 |         0 |        25 |        25 | 
## ----------------------------------|-----------|-----------|-----------|-----------|
##                      Column Total |        32 |        10 |        25 |        67 | 
## ----------------------------------|-----------|-----------|-----------|-----------|
## 
## </code></pre>
<p>The results of the confusion matrix indicate that our model predicts species very well. The numbers are all zeros in the off-diagonals apart from one case in which a penguin from the Chinstrap class has been classified as Adelie. This is indicating that our model has successfully classified the outcome based on the given predictors.</p>
<p>Next, we will run the KNN classification using the <code>caret</code> package which ‘authomatically’ picks the optimal number of neighbours <code>K</code>.</p>
<pre class="r"><code>### caret

pingo_knn_caret &lt;- train(class_pred_train, 
                         species_outcome_train,
                         method = &quot;knn&quot;, 
                         preProcess = c(&quot;center&quot;, &quot;scale&quot;))

pingo_knn_caret</code></pre>
<pre><code>## k-Nearest Neighbors 
## 
## 266 samples
##   8 predictor
##   3 classes: &#39;Adelie&#39;, &#39;Chinstrap&#39;, &#39;Gentoo&#39; 
## 
## Pre-processing: centered (7), scaled (7), ignore (1) 
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 266, 266, 266, 266, 266, 266, ... 
## Resampling results across tuning parameters:
## 
##   k  Accuracy   Kappa    
##   5  0.9904743  0.9849471
##   7  0.9934275  0.9897027
##   9  0.9930282  0.9890629
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 7.</code></pre>
<p>Looking at the output of the <code>caret</code> package KNN model, we can see that it chose <code>K = 7</code>, given that this was the number at which accuracy and kappa peaked. Kappa statistic is a measure of inter-rater reliability which is a more robust measure than a simple percent agreement calculation. However, its interpretation is not as intuitive and easy to comprehend as a single percentage measure. For more on the kappa statistic see the <a href="https://www.statisticshowto.com/cohens-kappa-statistic/">Statistic How To</a> website.</p>
<pre class="r"><code>plot(pingo_knn_caret)</code></pre>
<p><img src="/module4/Classification/_index.en_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Next, we compare our predicted values of <code>species</code> to our actual values. We also print the confusion matrix, which will give an indication of how well our model has predicted the actual values. In the <code>caret</code> package the confusion matrix output also shows overall model statistics and statistics by class.</p>
<p>We can see that we have obtained the same results as previously. Do you have any suggestions why that may be?</p>
<pre class="r"><code>knn_predict &lt;- predict(pingo_knn_caret, newdata = class_pred_test) 

confusionMatrix(knn_predict, as.factor(species_outcome_test$species_outcome_test))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Adelie Chinstrap Gentoo
##   Adelie        31         1      0
##   Chinstrap      0        10      0
##   Gentoo         0         0     25
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9851          
##                  95% CI : (0.9196, 0.9996)
##     No Information Rate : 0.4627          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9757          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: Adelie Class: Chinstrap Class: Gentoo
## Sensitivity                 1.0000           0.9091        1.0000
## Specificity                 0.9722           1.0000        1.0000
## Pos Pred Value              0.9688           1.0000        1.0000
## Neg Pred Value              1.0000           0.9825        1.0000
## Prevalence                  0.4627           0.1642        0.3731
## Detection Rate              0.4627           0.1493        0.3731
## Detection Prevalence        0.4776           0.1493        0.3731
## Balanced Accuracy           0.9861           0.9545        1.0000</code></pre>
<p>So far we have learnt how to prepare data for KNN as well as conduct k-NN classification. In the next section we will learn about another popular classification method: logistic regression.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>Logistic Regression</strong></font>
</p>
<p>Often in studies, we encounter outcomes that are not continuous, but instead fall into 1 of 2 categories. In these cases we have binary outcomes, variables that can have only two possible values:</p>
<ul>
<li>Measuring a performance labeled as <span class="math inline">\(Y\)</span>. Candidates are classified as “good” or “poor”, coded as 1 and 0 respectively, i.e. <span class="math inline">\(Y\)</span>=1 representing good performance and <span class="math inline">\(Y\)</span>=0 representing poor performance.</li>
<li>Risk factor for cancer: person has cancer (<span class="math inline">\(Y\)</span>=1), or does not (<span class="math inline">\(Y\)</span>=0)</li>
<li>Whether a political candidate is going to win an election: lose <span class="math inline">\(Y\)</span>=0, win <span class="math inline">\(Y\)</span>=1</li>
<li>‘Health’ of a business can be observed by monitoring the solvency of the firm: bankrupt <span class="math inline">\(Y\)</span>=0, solvent <span class="math inline">\(Y\)</span>=1</li>
</ul>
<p>Here <span class="math inline">\(Y\)</span> is the binary response variable, that is coded as 0 or 1 and rather than predicting these two values for binary response we try to model the probabilities that <span class="math inline">\(Y\)</span> takes one of these two values.</p>
<p>Let us examine one such example using <a href="https://tanjakec.github.io/mydata/Bankrupticies.csv">the bankruptcies data</a> available from 👉 <a href="https://tanjakec.github.io/mydata/Bankruptcies.csv" class="uri">https://tanjakec.github.io/mydata/Bankruptcies.csv</a>.</p>
<p><a href="https://tanjakec.github.io/mydata/Bankruptcies.csv"><code>Bankruptcies.csv</code></a> file contains data of the operating financial ratios of 66 firms which either went bankrupt or remained solvent after a 2 years period. The data is taken from the book <a href="https://www.wiley.com/en-us/Regression+Analysis+by+Example%2C+5th+Edition-p-9781118456248">“Regression Analysis by Example”</a> by <a href="http://people.stern.nyu.edu/schatter/chatterjee.html">Chatterjee</a> and <a href="http://www1.aucegypt.edu/faculty/hadi/">Hadi</a>. The variables are:</p>
<ul>
<li>three financial ratios (the explanatory variables; measured variables):
<ul>
<li><span class="math inline">\(X_1\)</span>: Retained Earnings / Total Assets</li>
<li><span class="math inline">\(X_2\)</span>: Earnings Before Interest and Taxes / Total Assets</li>
<li><span class="math inline">\(X_3\)</span>: Sales / Total Assets</li>
</ul></li>
<li>a binary response variable <span class="math inline">\(Y\)</span>:
<ul>
<li>0 - if bankrupt after 2 years</li>
<li>1 - if solvent after 2 years</li>
</ul></li>
</ul>
<pre class="r"><code>bankrup &lt;- read.csv(&quot;https://tanjakec.github.io/mydata/Bankruptcies.csv&quot;)
summary(bankrup)</code></pre>
<pre><code>##        Y             X1                X2                 X3       
##  Min.   :0.0   Min.   :-308.90   Min.   :-280.000   Min.   :0.100  
##  1st Qu.:0.0   1st Qu.: -39.05   1st Qu.: -17.675   1st Qu.:1.025  
##  Median :0.5   Median :   7.85   Median :   4.100   Median :1.550  
##  Mean   :0.5   Mean   : -13.63   Mean   :  -8.226   Mean   :1.721  
##  3rd Qu.:1.0   3rd Qu.:  35.75   3rd Qu.:  14.400   3rd Qu.:1.975  
##  Max.   :1.0   Max.   :  68.60   Max.   :  34.100   Max.   :6.700</code></pre>
<p>Since</p>
<ol style="list-style-type: decimal">
<li>linear regression expects a numeric response variable and</li>
<li>we are interested in the analysis of the risk, ie. the probability of a firm going bankrupt based on its Retained Earnings / Total Assets ratio (RE/TA ratio) figure</li>
</ol>
<p>it would be very attractive to be able to use the same modelling techniques as for linear models. We are going to do just that: fit a simple linear regression model to examine this relationship to see if it will work.</p>
<p><span class="math display">\[Y = b_0 + b_1X_3\]</span></p>
<pre class="r"><code>model_lm = lm(Y ~  X1, data = bankrup)
# plot a scatter diagram of Y vs X1
plot(Y ~  X1, data = bankrup, 
     col = &quot;orangered&quot;, pch = &quot;|&quot;, ylim = c(-0.2, 1.2),
     main = &quot;using linear regression function for binomial Y&quot;)
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
abline(model_lm, lwd = 3, col = &quot;navy&quot;)</code></pre>
<p><img src="/module4/Classification/_index.en_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Examining this plot we can make two obvious observations:</p>
<ol style="list-style-type: decimal">
<li><p>the higher the ration of RE/TA the better the chances is for the firm to be solvent</p></li>
<li><p>by analysing the risk we analyse a chance, a probability of a company being solvent based on the value of the RE/TA. Since <span class="math inline">\(Y\)</span> is limited to the values of 0 and 1, rather than predicting these two values we will try to model the probabilities <span class="math inline">\(p\)</span> that <span class="math inline">\(Y\)</span> takes one of these two values.</p></li>
</ol>
<p>Let <span class="math inline">\(p\)</span> denote the probability that <span class="math inline">\(Y\)</span> = 1 when <span class="math inline">\(X = x\)</span>. If we use the standard linear model to describe <span class="math inline">\(p\)</span>, then our model for the probability would be</p>
<p><span class="math display">\[p = P(Y = 1 | X = x) = b_0 + b_1x + e\]</span></p>
<p>Note that since <span class="math inline">\(p\)</span> is a probability it must lie between 0 and 1. It seems rational that <span class="math inline">\(X\hat{b}\)</span> is a reasonable estimate of <span class="math inline">\(P(Y=1∣X=x)\)</span>. Nonetheless, the plot has flagged a big issue with this model and that is that it has predicted probabilities less than 0.</p>
<p>As we can see the linear regression model does not work for this type of problem, for which we do not expect predictions that are off-scale values: below zero or above 1.</p>
<p>Apart from the fact that the linear function given is unbounded, and hence cannot be used to model probability, the other assumptions of linear regression when dealing with this type of a problem are also not valid:</p>
<ul>
<li>the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is nonlinear</li>
<li>error terms are not normally distributed</li>
<li>the assumption of equal/constant variance (homoscedasticity) dos not hold</li>
</ul>
<p>A workaround these issues is to fit a different model, one that is bounded by the minimum and maximum probabilities. It makes better sense to model the probabilities on a transformed scale and this is what is done in logistic regression analysis. The relationship between the probability <span class="math inline">\(p\)</span> and <span class="math inline">\(X\)</span> can be presented by a <strong>logistic regression function</strong>.</p>
<p><img src="/module4/Classification/_index.en_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>The shape of the S-curve given in the figure above can be reproduced if we model the probabilities as follows</p>
<p><span class="math display">\[p = P(Y = 1 | X = x) = \frac{e^{\beta_0 + \beta_1x}}{1 + e^{\beta_0 + \beta_1x}},\]</span></p>
<p>where <span class="math inline">\(e\)</span> is the base of the natural logarithm. The logistic model can be generalized directly to the situation where we have several predictor variables. The probability <span class="math inline">\(p\)</span> is modelled as</p>
<p><span class="math display">\[p = P(Y = 1 | X_1 = x_1, X_2=x_2, ..., X_q=x_q) = \frac{e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_qx_q}}{1 + e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_qx_q}},\]</span>
where <span class="math inline">\(q\)</span> is the number of predictors. The two equations above are called the logistic regression functions. It is nonlinear in the parameters <span class="math inline">\(\beta_0, \beta_1,... \beta_q\)</span>. However, it can be linearised by the <strong>logit transformation</strong>. Instead of working directly with <span class="math inline">\(p\)</span> we work with a transformed value of <span class="math inline">\(p\)</span>. If <span class="math inline">\(p\)</span> is the probability of an event happening, the ratio <span class="math inline">\(\frac{p}{(1-p)}\)</span> is called the <strong>odds ratio</strong> for the event. By moving some terms around</p>
<p><span class="math display">\[1 - p = P(Y = 1 | X_1 = x_1, X_2=x_2, ..., X_q=x_q) = \frac{1}{1 + e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_qx_q}},\]</span>
we get</p>
<p><span class="math display">\[\frac{p}{1-p} = e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_qx_q}\]</span>
Taking the natural logarithm of both sides of the above expression, we obtain</p>
<p><span class="math display">\[g(x_1, x_2, ... x_q) = log(\frac{p}{1-p}) = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_qx_q\]</span>
where the logarithm of the odds ratio is called the <strong>logit</strong>. We realise that the logit transformation produces a linear function of the parameters <span class="math inline">\(\beta_0, \beta_1,... \beta_q\)</span>. It is important to realise is that while the range of values of <span class="math inline">\(p\)</span> of binomial response <span class="math inline">\(Y\)</span> is between 0 and 1, the range of values of <span class="math inline">\(log\frac{p}{(1-p)}\)</span> is between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(+\infty\)</span>. This makes the logarithm of the odds ratio, known as logits, more appropriate for linear regression fitting.</p>
<p>In logistic regression the response probabilities are modelled by the logistic distribution function. That is, the model does not use the binned version of the predictor, but rather the log odds are modelled as a function of the predictor. The model parameters are estimated by working with logits which produces a model that is linear in the parameters.</p>
<p>The method of model estimation is the <strong>maximum likelihood</strong> method. Maximum likelihood parameter estimation is a technique that can be used when we can make assumptions about the probability distribution of the data. Based on the theoretical probability distribution and the observed data, the likelihood function is a probability statement that can be made about a particular set of parameter values. In logistic regression modelling the maximum likelihood parameters are obtained numerically using an interactive procedure. This is explained in the <a href="http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf">book by McCullagh and Nelder</a> in Section 4.4.2.<br />
Although the method of maximum likelihood is used for the model estimation we ask the same set of questions that are usually considered in linear regression. That is, we can not use the very familiar least square regression tools such as <span class="math inline">\(R^2\)</span>, <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span>, but that is not to say that we are not able to answer the same questions as we do when assessing a leaner regression model for which we use the listed tools.</p>
<p><a href="https://en.wikipedia.org/wiki/John_Nelder">John Nelder</a> and <a href="https://en.wikipedia.org/wiki/Robert_Wedderburn_(statistician)">Robert Wedderburn</a> formulated a modelling technique for accommodating response variables with non-normal conditional distribution. Logistic regression and ordinary linear regression fall into this larger class of techniques called <a href="https://docs.ufpr.br/~taconeli/CE225/Artigo.pdf"><strong>Generalised Linear Models</strong></a> (<strong>GLM</strong>s) which accommodate many different probability distributions. They substantially extend the range of application of linear statistical models by accommodating response variables with non-normal conditional distribution. Except for the error, the right hand side of a GLM model equation is basically the same as for a linear model. This is reflected in the syntax of the <code>glm()</code> function in R, which expects the formula that specifies the right-hand side of GLM to be the same as those used for the least square linear regression model, with the addition of description for the error distribution.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>Example in R </strong></font>
</p>
<p>In the following examples we will apply logistic regression in R by directly applying the <code>glm()</code> function and by using the <code>caret</code> package. We will use two data sets: <a href="https://tanjakec.github.io/mydata/Bankruptcies.csv"><code>Bankrupticies.csv</code></a> which we introduced earlier and the <code>CreditCard</code> data set from the <code>AER</code> package. This is cross-section data on the credit history for a sample of applicants for a type of credit card that consists of 1319 observations with 12 variables.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>Using the <code>glm()</code> function </strong></font>
</p>
<p>Through this example we will fit a logistic regression model using the <code>glom</code> function in order to learn how such models are fitted and evaluated.</p>
<p>We have already uploaded the <a href="https://tanjakec.github.io/mydata/Bankruptcies.csv"><code>Bankrupticies.csv</code></a> data file, which contains information of the operating financial ratios of 66 firms which either went bankrupt or remained solvent after a 2 years period earlier. Hence, the response variable is defined as</p>
<p><span class="math display">\[
Y =
  \begin{cases}
   0 &amp; \text{if bunkrupt after 2 year}} \\
   1       &amp; \text{if solvent after 2 years}
  \end{cases}
\]</span></p>
<p>Here, we will instead of predicting <span class="math inline">\(Y\)</span>, fit the model to predict the logits <span class="math inline">\(log\frac{p}{(1-p)}\)</span>, which after transformation means we can get the predicted probabilities for <span class="math inline">\(Y\)</span>.</p>
<p>Let us remind ourselves what the data looks like.</p>
<pre class="r"><code>suppressPackageStartupMessages(library(dplyr))

bankrup &lt;- read.csv(&quot;https://tanjakec.github.io/mydata/Bankruptcies.csv&quot;)
summary(bankrup)</code></pre>
<pre><code>##        Y             X1                X2                 X3       
##  Min.   :0.0   Min.   :-308.90   Min.   :-280.000   Min.   :0.100  
##  1st Qu.:0.0   1st Qu.: -39.05   1st Qu.: -17.675   1st Qu.:1.025  
##  Median :0.5   Median :   7.85   Median :   4.100   Median :1.550  
##  Mean   :0.5   Mean   : -13.63   Mean   :  -8.226   Mean   :1.721  
##  3rd Qu.:1.0   3rd Qu.:  35.75   3rd Qu.:  14.400   3rd Qu.:1.975  
##  Max.   :1.0   Max.   :  68.60   Max.   :  34.100   Max.   :6.700</code></pre>
<pre class="r"><code>glimpse(bankrup)</code></pre>
<pre><code>## Rows: 66
## Columns: 4
## $ Y  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ X1 &lt;dbl&gt; -62.8, 3.3, -120.8, -18.1, -3.8, -61.2, -20.3, -194.5, 20.8, -106.1…
## $ X2 &lt;dbl&gt; -89.5, -3.5, -103.2, -28.8, -50.6, -56.2, -17.4, -25.8, -4.3, -22.9…
## $ X3 &lt;dbl&gt; 1.7, 1.1, 2.5, 1.1, 0.9, 1.7, 1.0, 0.5, 1.0, 1.5, 1.2, 1.3, 0.8, 2.…</code></pre>
<p>Before we start the model fitting procedure we will make test-train split data in the proportion of 80:20.</p>
<pre class="r"><code>set.seed(123)
split_idx = sample(nrow(bankrup), 53)
bankrup_train = bankrup[split_idx, ]
bankrup_test = bankrup[-split_idx, ]</code></pre>
<p>We will have a quick glance at data to see what the training data is like</p>
<pre class="r"><code>glimpse(bankrup_train)</code></pre>
<pre><code>## Rows: 53
## Columns: 4
## $ Y  &lt;int&gt; 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1…
## $ X1 &lt;dbl&gt; -8.8, 31.4, 7.2, -120.8, 68.6, 18.1, 40.6, 37.3, 35.0, 21.5, 59.5, …
## $ X2 &lt;dbl&gt; -9.1, 15.7, -22.6, -103.2, 13.8, 13.5, 5.8, 33.4, 20.8, -14.4, 7.0,…
## $ X3 &lt;dbl&gt; 0.9, 1.9, 2.0, 2.5, 1.6, 4.0, 1.8, 3.5, 1.9, 1.0, 2.0, 2.0, 2.1, 2.…</code></pre>
<pre class="r"><code>summary(as.factor(bankrup_train$Y))</code></pre>
<pre><code>##  0  1 
## 28 25</code></pre>
<pre class="r"><code>summary(bankrup_train)</code></pre>
<pre><code>##        Y                X1                X2                X3      
##  Min.   :0.0000   Min.   :-308.90   Min.   :-280.00   Min.   :0.10  
##  1st Qu.:0.0000   1st Qu.: -39.40   1st Qu.: -20.80   1st Qu.:1.10  
##  Median :0.0000   Median :   3.30   Median :   1.60   Median :1.60  
##  Mean   :0.4717   Mean   : -17.83   Mean   : -11.93   Mean   :1.76  
##  3rd Qu.:1.0000   3rd Qu.:  35.00   3rd Qu.:  13.50   3rd Qu.:2.00  
##  Max.   :1.0000   Max.   :  68.60   Max.   :  33.40   Max.   :6.70</code></pre>
<p>We have a nice split in terms of number of observations for each class of Y. Next, we estimate a logistic regression model using the <code>glm()</code> (generalised linear model) function which we save as an object.</p>
<pre class="r"><code>model &lt;- glm(Y ~ X1 + X2 + X3, data = bankrup_train, family = binomial(logit))
model</code></pre>
<pre><code>## 
## Call:  glm(formula = Y ~ X1 + X2 + X3, family = binomial(logit), data = bankrup_train)
## 
## Coefficients:
## (Intercept)           X1           X2           X3  
##     -9.8846       0.3238       0.1779       4.9443  
## 
## Degrees of Freedom: 52 Total (i.e. Null);  49 Residual
## Null Deviance:       73.3 
## Residual Deviance: 5.787     AIC: 13.79</code></pre>
<p>In order to get all the results saved in the <code>glm_model</code> object we use the summary command.</p>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Y ~ X1 + X2 + X3, family = binomial(logit), data = bankrup_train)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.63381  -0.00039   0.00000   0.00098   1.41422  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)  -9.8846    10.6564  -0.928    0.354  
## X1            0.3238     0.2959   1.094    0.274  
## X2            0.1779     0.1079   1.650    0.099 .
## X3            4.9443     5.0231   0.984    0.325  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 73.3037  on 52  degrees of freedom
## Residual deviance:  5.7875  on 49  degrees of freedom
## AIC: 13.787
## 
## Number of Fisher Scoring iterations: 12</code></pre>
<blockquote>
<p>The key components of R’s summary( ) function for generalised linear models for binomial family with the logit link are:</p>
<ul>
<li><p><strong>Call</strong>: just like in the case of fitting the <code>lm()</code> model this is R reminding us what model we ran, what options we specified, etc</p></li>
<li><p>the <strong>Deviance Residuals</strong> are a measure of model fit. This part of the output shows the distribution of the deviance residuals for individual cases used in the model. Below we discuss how to use summaries of the deviance statistic to assess model fit</p></li>
<li><p>the <strong>Coefficients</strong>, their standard errors, the z-statistic (sometimes called a Wald z-statistic), and the associated p-values. The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable</p></li>
</ul>
<p>At the end there are <strong>fit indices</strong>, including the null and deviance residuals and the AIC, which are used to assess overall model fit.</p>
</blockquote>
<p>We realise that the output above has a resemblance to the standard regression output. Although they differ in the type of information, they serve similar functions. Let us make an interpretation of it.</p>
<p>The fitted logarithm of the odds ratio, i.e. logit of the probability <span class="math inline">\(p\)</span> of the firm remaining solvent after two years is modelled as:</p>
<p><span class="math display">\[\hat{g}(X_1, X_2, ...X_q) = -9.8846 +  0.3238X_1 + 0.1779X_2 + 4.9443X_3\]</span>
Remember that here instead of predicting <span class="math inline">\(Y\)</span> we obtain the model to predict <span class="math inline">\(log{frac{p}{(1-p)}\)</span>. Using the transformation of the logit we get the predicted probabilities of the firm being solvent.</p>
<p>The estimated parameters are expected changes in the logit for unit change in their corresponding variables when the other, remaining variables are held fixed. That is, the logistic regression coefficients give the change in the log odds of the response variable for a unit increase in the explanatory variable:</p>
<ul>
<li>For a unit change in X1, the log odds of a firm remaining solvent increases by 0.33, while the other variables, X2 and X3, are held fixed</li>
<li>For a unit change in X2, the log odds of a firm remaining solvent increases by 0.18, while the other variables, X1 and X3, are held fixed</li>
<li>For a unit change in X3, the log odds of a firm remaining solvent increases by 5.09, while the other variables, X1 and X2, are held fixed</li>
</ul>
<p>This is very hard to make sense of. We have predicted log odds and in order to interpret them into a more sensible fashion we need to “anti logge” them as the changes in odds ratio for a unit change in variable <span class="math inline">\(X_i\)</span>, while the other variables are held fixed <span class="math inline">\(e^(\beta_i)\)</span>.</p>
<pre class="r"><code>round(exp(coef(model)), 4)</code></pre>
<pre><code>## (Intercept)          X1          X2          X3 
##      0.0001      1.3823      1.1947    140.3731</code></pre>
<p>Now, we can interpret the coefficients of the rations as follows:</p>
<ul>
<li>The odds of a firm being solvent (vs being bankrupt) increases by 1.38 for a unit change in ratio <span class="math inline">\(X_1\)</span>, all else in the model being being fixed. That is, for an increase in <span class="math inline">\(X_1\)</span> the relative odds of
<span class="math display">\[\frac{P(Y=1)}{P(Y=0)}\]</span></li>
</ul>
<p>is multiplied by <span class="math inline">\(e^{\beta_1} = e^{0.3238} = 1.38\)</span>, implying that there is an increase of <span class="math inline">\(38\%\)</span></p>
<ul>
<li><p>The odd of a firm being solvent (vs being bankrupt) increases by 1.20 for a unit change in ratio <span class="math inline">\(X_2\)</span>, all else in the model being being fixed… implying that there is an increase of <span class="math inline">\(20\%\)</span></p></li>
<li><p>The odd of a firm being solvent (vs being bankrupt) increases by 140.37 for a unit change in ratio <span class="math inline">\(X_3\)</span>, all else in the model being being fixed… implying that there is an increase of <span class="math inline">\(40.37\%\)</span></p></li>
</ul>
<p>The column headed as <code>z value</code> is the ratio of the coefficients (<code>Estimate</code>) to their standard errors (<code>Std. Error</code>) known as <strong>Wald statistics</strong> for testing the hypothesis that the corresponding parameter are zeros. In standard regression this would be the <strong>t-test</strong>. Next to Wald test statistics we have their corresponding <span class="math inline">\(p\)</span>-values (<code>Pr(&gt;|z|)</code>) which are used to judge the significance of the coefficients. Values smaller than <span class="math inline">\(0.5\)</span> would lead to the conclusion that the coefficient is significantly different from <span class="math inline">\(0\)</span> at <span class="math inline">\(5\%\)</span> significant level. From the output obtained we see that none of the <span class="math inline">\(p\)</span>-values is smaller than <span class="math inline">\(0.5\)</span> implying that none of the variables individually is significant for predicting the logit of the observations.</p>
<p>We need to make a proper assessment to check if the variables collectively contribute in explaining the logit. That is, we need to examine whether the coefficients <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\beta_3\)</span> are all zeros. We do this using the <span class="math inline">\(G\)</span> statistic, which stands for <strong>goodness of fit</strong></p>
<p><span class="math display">\[G = \text{likelihood without the predictors}- \text{likelihood with the predictors}\]</span>
<span class="math inline">\(G\)</span> is distributed as a <em>chi-square</em> statistic with 1 degree of freedom, so a <em>chi-square test</em> is the test of the fit of the model (note, that <span class="math inline">\(G\)</span> is similar to an <span class="math inline">\(R^{2}\)</span> type test). The question we have now is where do we get this statistic from?</p>
<p>In addition to the above, the summary of the model includes the deviance and degrees of freedom for a model with only an intercept (the null deviance) and the residual deviance and degrees of freedom for a fitted model. We calculate the <span class="math inline">\(G\)</span> statistic as difference in deviances between the null model and the fitted model</p>
<p>Before we calculate the <span class="math inline">\(G\)</span> statistic we need to specify the hypotheses:</p>
<ul>
<li><span class="math inline">\(H_0: \beta_i = 0\)</span></li>
<li><span class="math inline">\(H_1: \text{at least one is different from}0\)</span></li>
</ul>
<p>where <span class="math inline">\(i = 1, 2, 3\)</span>. The decision rule is:</p>
<ul>
<li>If <span class="math inline">\(G_{calc} &lt; G_{crit} \Rightarrow H_0 \text{, o.w. } H_1\)</span></li>
</ul>
<p>We can also consider: If its associated <span class="math inline">\(p\)</span>-value is greater than <span class="math inline">\(0.05\)</span> we conclude that the variables do not collectively influence the logits, if however <span class="math inline">\(p\)</span>-value is less that <span class="math inline">\(0.05\)</span> we conclude that they do collectively influence the logits.</p>
<p>Next we calculate the <span class="math inline">\(G\)</span> statistic and the degrees of freedom of the corresponding critical value. Knowing what it is, we can calculated manually or obtain it using the <code>pscl::pR2()</code> function</p>
<pre class="r"><code>G_calc &lt;- model$null.deviance - model$deviance
Gdf &lt;- model$df.null - model$df.residual
pscl::pR2(model)</code></pre>
<pre><code>## fitting null model for pseudo-r2</code></pre>
<pre><code>##         llh     llhNull          G2    McFadden        r2ML        r2CU 
##  -2.8937311 -36.6518495  67.5162368   0.9210482   0.7202590   0.9613743</code></pre>
<pre class="r"><code>G_calc</code></pre>
<pre><code>## [1] 67.51624</code></pre>
<pre class="r"><code>qchisq(.95, df = Gdf) </code></pre>
<pre><code>## [1] 7.814728</code></pre>
<pre class="r"><code>1 - pchisq(G_calc, Gdf)</code></pre>
<pre><code>## [1] 1.454392e-14</code></pre>
<p>As <span class="math inline">\(G_{calc}\)</span> = 67.52 &gt; <span class="math inline">\(G_{crit}\)</span> = 7.81 <span class="math inline">\(\Rightarrow H_1\)</span>, ie. <span class="math inline">\(p\text{-value}\)</span> = 0.00 &lt; 0.05 we can conclude that this is a statistically valid model and that the variables collectively have explanatory power. The next question we need to ask is do we need all three variables?</p>
<p>In linear regression we assessed the significance of individual regression coefficients, i.e. the contribution of the individual variables using the <em>t-test</em>. Here, we use the <em>z scores</em> to conduct the equivalent Wald statistic (test). The ratio of the logistic regression has a normal distribution as opposed to the t-distribution we have seen in linear regression. Nonetheless, the set of hypotheses we wish to test is the same:</p>
<ul>
<li><span class="math inline">\(H_0: \beta_i = 0\)</span> (coefficient <span class="math inline">\(i\)</span> is not significant, thus <span class="math inline">\(X_i\)</span> is not important)</li>
<li><span class="math inline">\(H_1: \beta_i \neq 0\)</span> (coefficient <span class="math inline">\(i\)</span> is significant, thus <span class="math inline">\(X_i\)</span> is important)</li>
</ul>
<p>We are going to use a simplistic approach in testing these hypotheses in terms of the adopted decision rule. Decision Rule: If the Wald’s z statistic lies between -2 and +2, then the financial ratio, <span class="math inline">\(X_i\)</span>, is not needed and can be dropped from the analysis. Otherwise, we will keep the financial ratio. However, there is some scope here for subjective judgement depending upon how near to +/-2 theWald’s <span class="math inline">\(z\)</span> value is. Therefore we may have to do some “manual” searching upon an acceptable set of explanatory variables, as the z value of all three variables in the model lies between +/-2.</p>
<p>In our example none of the variables appear to be important judging upon their Wald’s z statstic, yet based on the chi-square statistic <span class="math inline">\(G\)</span>, we know that the fitted model is valid and that the selected variables collectively contribute in explaining the logits.</p>
<p>To evaluation individual contribution of the variables used in a logistic regression model we examine what happens to the change in the chi-square statistic <span class="math inline">\(G\)</span> when the <span class="math inline">\(i^{th}\)</span> variable is removed from the model. A large <span class="math inline">\(p\)</span>-value means that the reduced model explains about the same amount of variation as the full model and, thus, we can probably leave out the variable.</p>
<p>Decision Rule: if <span class="math inline">\(p\)</span>-value &gt; 0.05 <span class="math inline">\(\Rightarrow\)</span> the variable can be taken out, otherwise if <span class="math inline">\(p\)</span>-value &lt; 0.05 keep the variable in the model.</p>
<p>Rather then fitting individual models and doing a manual comparison we can make use of the anova function for comparing the nested model using the chi-square test.</p>
<pre class="r"><code>anova(model, test=&quot;Chisq&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: Y
## 
## Terms added sequentially (first to last)
## 
## 
##      Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
## NULL                    52     73.304              
## X1    1   58.258        51     15.045 2.298e-14 ***
## X2    1    5.728        50      9.317   0.01670 *  
## X3    1    3.530        49      5.787   0.06028 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can remove <span class="math inline">\(X_3\)</span> variable from the model</p>
<pre class="r"><code>model_new &lt;- update(model, ~. -X3, data = bankrup)
summary(model_new)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Y ~ X1 + X2, family = binomial(logit), data = bankrup)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.01332  -0.00643   0.00095   0.01421   1.30308  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.55034    0.95101  -0.579   0.5628  
## X1           0.15736    0.07492   2.100   0.0357 *
## X2           0.19474    0.12244   1.591   0.1117  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 91.4954  on 65  degrees of freedom
## Residual deviance:  9.4719  on 63  degrees of freedom
## AIC: 15.472
## 
## Number of Fisher Scoring iterations: 10</code></pre>
<pre class="r"><code>anova(model_new, test=&quot;Chisq&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: Y
## 
## Terms added sequentially (first to last)
## 
## 
##      Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
## NULL                    65     91.495             
## X1    1   75.692        64     15.803  &lt; 2e-16 ***
## X2    1    6.331        63      9.472  0.01186 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>To compare the fit of the alternative model we will use the <strong>A</strong>kaike <strong>I</strong>nformation <strong>C</strong>riterion (<strong>AIC</strong>), which is an index of fit that takes account of parsimony of the model by penalising for the number of parameters. It is defined as</p>
<p><span class="math display">\[AIC = - 2 \times \text{maximum log-likelihood} + 2 \times \text{number of parameters}\]</span>
and thus smaller values are indicative of a better fit to the data. In the context of logit fit, the AIC is simply the residual deviance plus twice the number of regression coefficients.</p>
<p>The AIC of the initial model is 13.787 and of the new model 15.472. Checking the new model, we can see that it consists of the variables that all significantly contribute in explaining the logits. So, in the spirit of parsimony we can choose the second model to be a better fit.</p>
<p>To obtain the overall accuracy rate we need to find the predicted probabilities of the observations kept aside in the <code>bankrup_test</code> subset. By default, <code>predict.glm()</code> uses <code>type = "link"</code> and if not specified otherwise R is returning:</p>
<p><span class="math display">\[\hat{\beta_0} + \hat{\beta_1}X_1 + \hat{\beta_2}X_2\]</span>
for each observation, which are not predicted probabilities. To obtain the predicted probabilities:</p>
<p><span class="math display">\[\frac{1}{1 + e^{\hat{\beta_0} + \hat{\beta}_1x_1 + \hat{\beta}_2x_2}}\]</span>
when using the <code>predict.glm()</code> function we need to use <code>type = "response"</code>.</p>
<pre class="r"><code>link_pr &lt;- round(predict(model_new,  bankrup_test, type = &quot;link&quot;), 2)
link_pr</code></pre>
<pre><code>##      4     20     22     24     30     38     40     44     46     47     55 
##  -9.01 -11.64  -4.66 -23.62  -9.53   9.25   9.24  13.23   2.78  11.96  10.04 
##     58     63 
##  10.90   8.40</code></pre>
<pre class="r"><code>response_pr &lt;- round(predict(model_new,  bankrup_test, type = &quot;response&quot;), 2)

t(bankrup_test$Y)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## [1,]    0    0    0    0    0    1    1    1    1     1     1     1     1</code></pre>
<pre class="r"><code>coefficients(model_new)</code></pre>
<pre><code>## (Intercept)          X1          X2 
##  -0.5503398   0.1573639   0.1947428</code></pre>
<p>Here we illustrate how these predictions are made for the third and twelfth observations in the <code>bankrup_test</code> data subset.</p>
<pre class="r"><code>bankrup_test[c(3, 12),]</code></pre>
<pre><code>##    Y    X1   X2  X3
## 22 0 -18.1 -6.5 0.9
## 58 1  54.7 14.6 1.7</code></pre>
<pre class="r"><code>round(1/(1+exp(-(-0.5503398 + 0.1573639*(-18.1) + 0.1947428*(-6.5)))), 4)</code></pre>
<pre><code>## [1] 0.0093</code></pre>
<pre class="r"><code>round(1/(1+exp(-(-0.5503398 + 0.1573639*54.7 + 0.1947428*14.6))), 4)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>Knowing how the probabilities are estimated for the test data we can now describe the performance of a classification model using the confusion matrix.</p>
<pre class="r"><code>how_well &lt;- data.frame(response_pr, bankrup_test$Y) %&gt;% 
  mutate(result = round(response_pr) == bankrup_test$Y)

how_well</code></pre>
<pre><code>##    response_pr bankrup_test.Y result
## 4         0.00              0   TRUE
## 20        0.00              0   TRUE
## 22        0.01              0   TRUE
## 24        0.00              0   TRUE
## 30        0.00              0   TRUE
## 38        1.00              1   TRUE
## 40        1.00              1   TRUE
## 44        1.00              1   TRUE
## 46        0.94              1   TRUE
## 47        1.00              1   TRUE
## 55        1.00              1   TRUE
## 58        1.00              1   TRUE
## 63        1.00              1   TRUE</code></pre>
<pre class="r"><code>confusion_matrix &lt;- table(bankrup_test$Y, round(response_pr))
confusion_matrix</code></pre>
<pre><code>##    
##     0 1
##   0 5 0
##   1 0 8</code></pre>
<pre class="r"><code>accuracy(confusion_matrix) # accuracy() function we created for KNN classification earlier</code></pre>
<pre><code>## [1] 100</code></pre>
<p>The results above are showing that our chosen logistic regression model has correctly classified all firms from the <code>bankrup_test</code> data subset, giving an accuracy of 100%.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>Using the <code>caret</code> package</strong></font>
</p>
<pre class="r"><code># If you don&#39;t have AER installed yet, uncomment and run the lines below
#install.packages(&quot;AER&quot;)
suppressPackageStartupMessages(library(AER))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(caret))
data(&quot;CreditCard&quot;)
glimpse(CreditCard)</code></pre>
<pre><code>## Rows: 1,319
## Columns: 12
## $ card        &lt;fct&gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, no,…
## $ reports     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 3…
## $ age         &lt;dbl&gt; 37.66667, 33.25000, 33.66667, 30.50000, 32.16667, 23.25000…
## $ income      &lt;dbl&gt; 4.5200, 2.4200, 4.5000, 2.5400, 9.7867, 2.5000, 3.9600, 2.…
## $ share       &lt;dbl&gt; 0.0332699100, 0.0052169420, 0.0041555560, 0.0652137800, 0.…
## $ expenditure &lt;dbl&gt; 124.983300, 9.854167, 15.000000, 137.869200, 546.503300, 9…
## $ owner       &lt;fct&gt; yes, no, yes, no, yes, no, no, yes, yes, no, yes, yes, yes…
## $ selfemp     &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, no…
## $ dependents  &lt;dbl&gt; 3, 3, 4, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 0, 2, 1, 2, 2, 0, 0…
## $ months      &lt;dbl&gt; 54, 34, 58, 25, 64, 54, 7, 77, 97, 65, 24, 36, 42, 26, 120…
## $ majorcards  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1…
## $ active      &lt;dbl&gt; 12, 13, 5, 7, 5, 1, 5, 3, 6, 18, 20, 0, 12, 3, 5, 22, 0, 8…</code></pre>
<pre class="r"><code>summary(CreditCard)</code></pre>
<pre><code>##   card         reports             age              income      
##  no : 296   Min.   : 0.0000   Min.   : 0.1667   Min.   : 0.210  
##  yes:1023   1st Qu.: 0.0000   1st Qu.:25.4167   1st Qu.: 2.244  
##             Median : 0.0000   Median :31.2500   Median : 2.900  
##             Mean   : 0.4564   Mean   :33.2131   Mean   : 3.365  
##             3rd Qu.: 0.0000   3rd Qu.:39.4167   3rd Qu.: 4.000  
##             Max.   :14.0000   Max.   :83.5000   Max.   :13.500  
##      share            expenditure       owner     selfemp      dependents    
##  Min.   :0.0001091   Min.   :   0.000   no :738   no :1228   Min.   :0.0000  
##  1st Qu.:0.0023159   1st Qu.:   4.583   yes:581   yes:  91   1st Qu.:0.0000  
##  Median :0.0388272   Median : 101.298                        Median :1.0000  
##  Mean   :0.0687322   Mean   : 185.057                        Mean   :0.9939  
##  3rd Qu.:0.0936168   3rd Qu.: 249.036                        3rd Qu.:2.0000  
##  Max.   :0.9063205   Max.   :3099.505                        Max.   :6.0000  
##      months         majorcards         active      
##  Min.   :  0.00   Min.   :0.0000   Min.   : 0.000  
##  1st Qu.: 12.00   1st Qu.:1.0000   1st Qu.: 2.000  
##  Median : 30.00   Median :1.0000   Median : 6.000  
##  Mean   : 55.27   Mean   :0.8173   Mean   : 6.997  
##  3rd Qu.: 72.00   3rd Qu.:1.0000   3rd Qu.:11.000  
##  Max.   :540.00   Max.   :1.0000   Max.   :46.000</code></pre>
<p>The <code>CreditCard</code> data contains a response variable <code>card</code> that indicates whether a consumer was declined (<code>no</code>) or approved for a credit card (<code>yes</code>). The first step is to partition the data into training and testing sets.</p>
<pre class="r"><code>#CreditCard$Class &lt;- CreditCard$card
#CreditCard &lt;- subset(CreditCard, select=-c(card, expenditure, share))
summary(CreditCard) </code></pre>
<pre><code>##   card         reports             age              income      
##  no : 296   Min.   : 0.0000   Min.   : 0.1667   Min.   : 0.210  
##  yes:1023   1st Qu.: 0.0000   1st Qu.:25.4167   1st Qu.: 2.244  
##             Median : 0.0000   Median :31.2500   Median : 2.900  
##             Mean   : 0.4564   Mean   :33.2131   Mean   : 3.365  
##             3rd Qu.: 0.0000   3rd Qu.:39.4167   3rd Qu.: 4.000  
##             Max.   :14.0000   Max.   :83.5000   Max.   :13.500  
##      share            expenditure       owner     selfemp      dependents    
##  Min.   :0.0001091   Min.   :   0.000   no :738   no :1228   Min.   :0.0000  
##  1st Qu.:0.0023159   1st Qu.:   4.583   yes:581   yes:  91   1st Qu.:0.0000  
##  Median :0.0388272   Median : 101.298                        Median :1.0000  
##  Mean   :0.0687322   Mean   : 185.057                        Mean   :0.9939  
##  3rd Qu.:0.0936168   3rd Qu.: 249.036                        3rd Qu.:2.0000  
##  Max.   :0.9063205   Max.   :3099.505                        Max.   :6.0000  
##      months         majorcards         active      
##  Min.   :  0.00   Min.   :0.0000   Min.   : 0.000  
##  1st Qu.: 12.00   1st Qu.:1.0000   1st Qu.: 2.000  
##  Median : 30.00   Median :1.0000   Median : 6.000  
##  Mean   : 55.27   Mean   :0.8173   Mean   : 6.997  
##  3rd Qu.: 72.00   3rd Qu.:1.0000   3rd Qu.:11.000  
##  Max.   :540.00   Max.   :1.0000   Max.   :46.000</code></pre>
<pre class="r"><code>set.seed(123)
train_ind &lt;- createDataPartition(CreditCard$card, p = 0.75, list = FALSE)

train_data &lt;- CreditCard[train_ind, ]
test_data &lt;- CreditCard[-train_ind, ]</code></pre>
<p>Although it might appear that there is no difference in splitting data using the <code>caret</code>’s <code>createDataPartition()</code> function from the previously used <code>sample()</code>, <code>createDataPartition()</code> tries to ensure a split that has a similar distribution of the supplied variable in both datasets. Next we can begin training a model. To do this we have to supply four arguments to the <code>train()</code> function</p>
<ul>
<li><code>form = card ~ .</code> specifies the response variable. It also indicates that all available predictors should be used</li>
<li><code>data = card_trn</code> specifies the training data</li>
<li><code>trControl = trainControl(method = "cv", number = 5)</code> specifies that we will be using 5-fold cross-validation</li>
<li><code>method = "glm"</code> specifies that we will fit a generalised linear model</li>
</ul>
<pre class="r"><code>credit_glm_mod = train(
  form = card ~ .,
  data = train_data,
  trControl = trainControl(method = &quot;cv&quot;, number = 5),
  method = &quot;glm&quot;,
  family = &quot;binomial&quot;
)</code></pre>
<p>The list that we have passed to the <code>trControl</code> argument is created using the <code>trainControl()</code> function from <code>caret</code>, specifying a number of training choices for the particular resampling scheme required for model training.</p>
<pre class="r"><code>str(trainControl(method = &quot;cv&quot;, number = 5))</code></pre>
<pre><code>## List of 27
##  $ method           : chr &quot;cv&quot;
##  $ number           : num 5
##  $ repeats          : logi NA
##  $ search           : chr &quot;grid&quot;
##  $ p                : num 0.75
##  $ initialWindow    : NULL
##  $ horizon          : num 1
##  $ fixedWindow      : logi TRUE
##  $ skip             : num 0
##  $ verboseIter      : logi FALSE
##  $ returnData       : logi TRUE
##  $ returnResamp     : chr &quot;final&quot;
##  $ savePredictions  : logi FALSE
##  $ classProbs       : logi FALSE
##  $ summaryFunction  :function (data, lev = NULL, model = NULL)  
##  $ selectionFunction: chr &quot;best&quot;
##  $ preProcOptions   :List of 6
##   ..$ thresh   : num 0.95
##   ..$ ICAcomp  : num 3
##   ..$ k        : num 5
##   ..$ freqCut  : num 19
##   ..$ uniqueCut: num 10
##   ..$ cutoff   : num 0.9
##  $ sampling         : NULL
##  $ index            : NULL
##  $ indexOut         : NULL
##  $ indexFinal       : NULL
##  $ timingSamps      : num 0
##  $ predictionBounds : logi [1:2] FALSE FALSE
##  $ seeds            : logi NA
##  $ adaptive         :List of 4
##   ..$ min     : num 5
##   ..$ alpha   : num 0.05
##   ..$ method  : chr &quot;gls&quot;
##   ..$ complete: logi TRUE
##  $ trim             : logi FALSE
##  $ allowParallel    : logi TRUE</code></pre>
<p>The elements of this list which are of most interest to us when setting up a model like the one above are the first three that are directly related to how the resampling will be done:</p>
<ul>
<li><code>method</code> specifies how resampling will be done: <code>cv</code>, <code>boot</code>, <code>LOOCV</code>, <code>repeatedcv</code>, and <code>oob</code></li>
<li><code>number</code> specifies the number of times resampling should be done for methods that require resample: <code>cv</code> and <code>boot</code></li>
<li><code>repeats</code> specifies the number of times to repeat resampling for methods such as <code>repeatedcv</code></li>
</ul>
<p>The additional argument <code>family</code> which is set to <code>"binomial"</code> is not actually an argument for <code>train()</code>, but an additional argument for the <code>method</code> which is chosen for <code>glm</code>. For a factor response variable, <code>caret</code> will recognise that we are trying to perform classification and will automatically use family = “binomial” by default. Nonetheless, for the purpose of easy readability of the code it is advisable for the <code>family</code> argument to be specified.</p>
<p>For details on the full model training and tuning see chapter 5 of <a href="https://twitter.com/topepos">Max’s</a> book <a href="https://topepo.github.io/caret/index.html">The caret Package</a>.</p>
<p>Calling the stored <code>train()</code> object, <code>credit_glm_mod</code>, will provide the summary of the training that has been done.</p>
<pre class="r"><code>credit_glm_mod</code></pre>
<pre><code>## Generalized Linear Model 
## 
## 990 samples
##  11 predictor
##   2 classes: &#39;no&#39;, &#39;yes&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 791, 793, 792, 791, 793 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.8767493  0.6583304</code></pre>
<p>It reports the statistical method used for the model estimation, which in our case is <code>glm</code>. We see that we used 990 observations that had a binary class response: “no” and “yes”, and eleven predictors. We have not done any data pre-processing, and have utilized 5-fold cross-validation. The cross-validated accuracy is reported as a proportion of correct classifications, which in our case is 87.67%. The stored <code>train()</code> object returns a list packed with useful information about the model’s training.</p>
<pre class="r"><code>names(credit_glm_mod)</code></pre>
<pre><code>##  [1] &quot;method&quot;       &quot;modelInfo&quot;    &quot;modelType&quot;    &quot;results&quot;      &quot;pred&quot;        
##  [6] &quot;bestTune&quot;     &quot;call&quot;         &quot;dots&quot;         &quot;metric&quot;       &quot;control&quot;     
## [11] &quot;finalModel&quot;   &quot;preProcess&quot;   &quot;trainingData&quot; &quot;resample&quot;     &quot;resampledCM&quot; 
## [16] &quot;perfNames&quot;    &quot;maximize&quot;     &quot;yLimits&quot;      &quot;times&quot;        &quot;levels&quot;      
## [21] &quot;terms&quot;        &quot;coefnames&quot;    &quot;contrasts&quot;    &quot;xlevels&quot;</code></pre>
<p>Two elements that we will be most interested in are <code>finalModel</code> and <code>results</code>.</p>
<pre class="r"><code>credit_glm_mod$finalModel</code></pre>
<pre><code>## 
## Call:  NULL
## 
## Coefficients:
## (Intercept)      reports          age       income        share  expenditure  
##  -1.003e+15   -4.338e+14    1.091e+12    7.486e+13    2.213e+16   -1.269e+12  
##    owneryes   selfempyes   dependents       months   majorcards       active  
##   3.613e+14   -1.884e+14   -1.102e+14   -5.498e+11    1.445e+14    4.367e+13  
## 
## Degrees of Freedom: 989 Total (i.e. Null);  978 Residual
## Null Deviance:       1054 
## Residual Deviance: 9732  AIC: 9756</code></pre>
<p>The <code>finalModel</code> is the object returned from <code>glm()</code>. This final model, is <strong>fit to all of the supplied training data</strong>. This model object is often used when we call certain relevant functions on the object returned by <code>train()</code>, one of which is <code>summary()</code>.</p>
<pre class="r"><code>credit_glm_mod$results</code></pre>
<pre><code>##   parameter  Accuracy     Kappa AccuracySD   KappaSD
## 1      none 0.8767493 0.6583304 0.06647345 0.1677757</code></pre>
<p>The <code>result</code> element shows more detailed results, in addition to the model’s overall accuracy. It provides the information of an estimate of the uncertainty in our accuracy estimate.</p>
<p>Obtaining the predictions on the test data with the object returned by <code>train()</code> is easy.</p>
<pre class="r"><code>pred &lt;- predict(credit_glm_mod, newdata = test_data)
head(pred, n = 10)</code></pre>
<pre><code>##  [1] yes yes no  no  yes yes yes yes no  yes
## Levels: no yes</code></pre>
<p>By default, the <code>predict()</code> function is returning classifications and this will be the case regardless of the matter being used. If instead of the default returning classifications, we want predicted probabilities, we simply specify <code>type = "prob"</code>.</p>
<pre class="r"><code>pred_prob &lt;- round(predict(credit_glm_mod, newdata = test_data, type = &quot;prob&quot;), 2)
head(pred_prob, n=10)</code></pre>
<pre><code>##    no yes
## 1   0   1
## 4   0   1
## 7   1   0
## 13  1   0
## 14  0   1
## 15  0   1
## 16  0   1
## 17  0   1
## 18  1   0
## 28  0   1</code></pre>
<p>the <code>predict()</code> function for a <code>train()</code> object will return the probabilities for all possible classes, in this case “no” and “yes”. This will be true for all methods, which is especially useful for multi-class data.</p>
<p>To summarise the performance of the trained model we will use the <code>confusionMatrix()</code> function to see how well the model does in predicting the target variable on out of sample observations.</p>
<pre class="r"><code>cm &lt;- confusionMatrix(data = pred, test_data$card)
cm</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  no yes
##        no   72  43
##        yes   2 212
##                                           
##                Accuracy : 0.8632          
##                  95% CI : (0.8213, 0.8985)
##     No Information Rate : 0.7751          
##     P-Value [Acc &gt; NIR] : 3.703e-05       
##                                           
##                   Kappa : 0.6722          
##                                           
##  Mcnemar&#39;s Test P-Value : 2.479e-09       
##                                           
##             Sensitivity : 0.9730          
##             Specificity : 0.8314          
##          Pos Pred Value : 0.6261          
##          Neg Pred Value : 0.9907          
##              Prevalence : 0.2249          
##          Detection Rate : 0.2188          
##    Detection Prevalence : 0.3495          
##       Balanced Accuracy : 0.9022          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<p>The accuracy of the model measured when using the unseen test data is 86%. That is, out of 329 observations in the test subset the model has correctly classified them 284 times. There is a scope for improvement. We have eleven predictors in this model and it would be useful to find out how well they contribute to the model’s performance. To assess the relative importance of individual predictors in the model using the <code>caret</code> package we can utilise the <code>varImp()</code> function.</p>
<pre class="r"><code>glm_Imp &lt;- varImp(credit_glm_mod)
plot(glm_Imp)</code></pre>
<p><img src="/module4/Classification/_index.en_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>The function automatically scales the importance scores to be between 0 and 100. Using <code>scale = FALSE</code> this normalisation step can be avoided.</p>
<p>Unfortunately, since logistic regression has no tuning parameters we can not take advantage of the <code>caret</code>’s feature selection facility (see <a href="https://r-forge.r-project.org/scm/viewvc.php/*checkout*/pkg/caret/inst/doc/caretSelection.pdf?revision=77&amp;root=caret&amp;pathrev=90">Variable Selection Using The caret Package</a>).</p>
<p>We will simple retrain the model removing <code>age</code> from the list of predictors. To ensure that each training gets the same data partitions and repeats we need to use the same seed number.</p>
<pre class="r"><code>set.seed(123)
credit_glm_mod_1 = train(
  form = card ~ . - age,
  data = train_data,
  trControl = trainControl(method = &quot;cv&quot;, number = 5),
  method = &quot;glm&quot;,
  family = &quot;binomial&quot;
)

credit_glm_mod_1</code></pre>
<pre><code>## Generalized Linear Model 
## 
## 990 samples
##  11 predictor
##   2 classes: &#39;no&#39;, &#39;yes&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 792, 792, 791, 793, 792 
## Resampling results:
## 
##   Accuracy  Kappa   
##   0.885766  0.731973</code></pre>
<pre class="r"><code>summary(credit_glm_mod_1)</code></pre>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
##  -8.49    0.00    0.00    0.00    8.49  
## 
## Coefficients:
##               Estimate Std. Error    z value Pr(&gt;|z|)    
## (Intercept) -1.018e+15  7.384e+06 -137927087   &lt;2e-16 ***
## reports     -4.456e+14  1.611e+06 -276618901   &lt;2e-16 ***
## income       6.523e+13  1.678e+06   38887709   &lt;2e-16 ***
## share        2.292e+16  4.973e+07  460874740   &lt;2e-16 ***
## expenditure -1.527e+12  1.713e+04  -89123688   &lt;2e-16 ***
## owneryes     3.139e+14  4.966e+06   63203076   &lt;2e-16 ***
## selfempyes  -3.688e+14  8.173e+06  -45120166   &lt;2e-16 ***
## dependents  -9.319e+13  1.861e+06  -50084127   &lt;2e-16 ***
## months      -2.331e+11  3.406e+04   -6845424   &lt;2e-16 ***
## majorcards   2.174e+14  5.588e+06   38899415   &lt;2e-16 ***
## active       4.784e+13  3.637e+05  131530123   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1053.8  on 989  degrees of freedom
## Residual deviance: 9803.9  on 979  degrees of freedom
## AIC: 9825.9
## 
## Number of Fisher Scoring iterations: 25</code></pre>
<p>Examining the output we can see that the accuracy has increased.</p>
<pre class="r"><code>glm_Imp_1 &lt;- varImp(credit_glm_mod_1)
plot(glm_Imp_1)</code></pre>
<p><img src="/module4/Classification/_index.en_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<p>The plot above indicates that predictor <code>months</code> has the lowest impact on the overall model performance.</p>
<p>To compare accuracy between the models we can resample the results of the trained models and compare their accuracy distributions using <code>caret</code>’s <code>resample</code> function and visualise them with the <code>dplot()</code> function. Although logistic regression has no tuning parameters, resampling can still be used to characterise the performance of the model.</p>
<pre class="r"><code>set.seed(123) # same resampling specification is used and, since the same random number seed is set
credit_glm_mod$results</code></pre>
<pre><code>##   parameter  Accuracy     Kappa AccuracySD   KappaSD
## 1      none 0.8767493 0.6583304 0.06647345 0.1677757</code></pre>
<pre class="r"><code>credit_glm_mod_1$results</code></pre>
<pre><code>##   parameter Accuracy    Kappa AccuracySD   KappaSD
## 1      none 0.885766 0.731973 0.08711903 0.1818121</code></pre>
<pre class="r"><code>results &lt;- resamples(list(M0 = credit_glm_mod, M1 = credit_glm_mod_1))
summary(results)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = results)
## 
## Models: M0, M1 
## Number of resamples: 5 
## 
## Accuracy 
##         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## M0 0.8121827 0.8341709 0.8743719 0.8767493 0.8781726 0.9848485    0
## M1 0.7676768 0.8383838 0.8832487 0.8857660 0.9646465 0.9748744    0
## 
## Kappa 
##         Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## M0 0.5617898 0.5781585 0.5809996 0.6583304 0.6141825 0.9565217    0
## M1 0.5082604 0.6190476 0.6999139 0.7319730 0.9017161 0.9309268    0</code></pre>
<pre class="r"><code>dotplot(results)</code></pre>
<p><img src="/module4/Classification/_index.en_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>However, if a strong claim needs to be made about which model is better, we could use statistical hypothesis tests to statistically check if the differences in the results are significant.</p>
<pre class="r"><code>modelDifferences &lt;- diff(results)
summary(modelDifferences)</code></pre>
<pre><code>## 
## Call:
## summary.diff.resamples(object = modelDifferences)
## 
## p-value adjustment: bonferroni 
## Upper diagonal: estimates of the difference
## Lower diagonal: p-value for H0: difference = 0
## 
## Accuracy 
##    M0     M1       
## M0        -0.009017
## M1 0.8479          
## 
## Kappa 
##    M0     M1      
## M0        -0.07364
## M1 0.3436</code></pre>
<p>The <span class="math inline">\(p\)</span>-values for the model comparisons are large (0.85 for accuracy and 0.34 for Kappa) indicating that the models fail to show any difference in performance.</p>
<p>
<font color="black" face="Verdana, Geneva, sans-serif" size="+1.5"><strong>YOUR TURN 👇</strong></font>
</p>
<p>Practise by doing the following exercises:</p>
<p>Access the <code>stroke</code> data set from <a href="https://tanjakec.github.io/mydata/stroke.csv" class="uri">https://tanjakec.github.io/mydata/stroke.csv</a>. Data dictionary of this data set is given in <a href="https://tanjakec.github.io/mydata/Stroke_DataDictionary.docx" class="uri">https://tanjakec.github.io/mydata/Stroke_DataDictionary.docx</a> word document.</p>
<ol style="list-style-type: lower-roman">
<li><p>Split the data into a training and a test set, pre-process the data, and train a model a model of your choice from this section</p></li>
<li><p>Predict the response for the test set. What is the value of the performance metric?</p></li>
<li><p>Which predictors are most important in the model you have trained?</p></li>
<li><p>Train the model using a different method from the previous selection. Don’t forget to initialise the same random number seed as the one used for the previous model training.</p></li>
<li><p>Compare the accuracy between the trained models. Which one does it perform better?</p></li>
</ol>
<hr />
<p>Further reading:</p>
<p><a href="http://appliedpredictivemodeling.com">“Applied Predictive Modeling”</a> by <a href="https://twitter.com/topepos">Max Kuhn</a></p>
<p><a href="https://www.statlearning.com">“Introduction to Statistical Learning with Applications in R” by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani</a></p>
<p><a href="https://twitter.com/topepos">Max Kuhn</a> <a href="https://topepo.github.io/caret/">The caret Package</a></p>
<hr />
<p>Useful links:</p>
<p>Check <a href="https://stats.idre.ucla.edu/r/dae/logit-regression/">Logistic Regression | R Data Analysis Example</a> which uses categorical predictors.</p>
<p><a href="http://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/GLM.logistic.Rpackage.pdf">Binary Response and Logistic Regression Analysis</a></p>
<p><a href="https://www.r-project.org/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf">Predictive Modeling with R and the caret Package, useR! 2013, by Max Kuhn</a></p>
<p><a href="https://www.youtube.com/watch?v=7Jbb2ItbTC4&amp;t=279s"><code>caret</code> package webinar by Max Khun</a></p>
<p><a href="https://www.youtube.com/watch?v=z8PRU46I3NY">Intro to Machine Learning with R &amp; caret</a></p>
<p>Data to practise:</p>
<p><a href="http://archive.ics.uci.edu/ml/index.php" class="uri">http://archive.ics.uci.edu/ml/index.php</a></p>
<p><a href="https://www.kdnuggets.com/datasets/index.html">KDnuggets data sets</a></p>
<p><a href="https://www.kaggle.com/datasets%3E">kaggle data sets</a></p>
<p><a href="https://github.com/chanmad/Machine-Learning-on-Bankruptcy">Machine Learning on Bankruptcy</a></p>
<p><a href="https://www.theguardian.com/data">The Guardian Data Journalism</a></p>
<hr />
<p>© 2021 Tatjana Kecojevic</p>





<footer class=" footline" >
	
</footer>

        
        </div> 
        

      </div>

    <div id="navigation">
        
        
        
        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                        
                    
                    

                    
                        
            
            
                
                    
                        
                        
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                
                

                    
                    
                    

                    
            
        
                    
                        
            
            
                
                    
                    
                
                

                    
                    
                    

                    
            
        
                    
            
        
                    
            
        
        
        


	 
	 
		
			<a class="nav nav-prev" href="/module4/subsets/" title="Subset Variable Selection"> <i class="fa fa-chevron-left"></i></a>
		
		
			<a class="nav nav-next" href="/module4/what_is_ml/" title="Machine Learning" style="margin-right: 0px;"><i class="fa fa-chevron-right"></i></a>
		
	
    </div>

    </section>
    
    <div style="left: -1000px; overflow: scroll; position: absolute; top: -1000px; border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;">
      <div style="border: none; box-sizing: content-box; height: 200px; margin: 0px; padding: 0px; width: 200px;"></div>
    </div>
    <script src="/js/clipboard.min.js?1638429882"></script>
    <script src="/js/perfect-scrollbar.min.js?1638429882"></script>
    <script src="/js/perfect-scrollbar.jquery.min.js?1638429882"></script>
    <script src="/js/jquery.sticky.js?1638429882"></script>
    <script src="/js/featherlight.min.js?1638429882"></script>
    <script src="/js/highlight.pack.js?1638429882"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="/js/modernizr.custom-3.6.0.js?1638429882"></script>
    <script src="/js/learn.js?1638429882"></script>
    <script src="/js/hugo-learn.js?1638429882"></script>

    <link href="/mermaid/mermaid.css?1638429882" rel="stylesheet" />
    <script src="/mermaid/mermaid.js?1638429882"></script>
    <script>
        mermaid.initialize({ startOnLoad: true });
    </script>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-105947713-1', 'auto');
  ga('send', 'pageview');

</script>
  </body>
</html>
